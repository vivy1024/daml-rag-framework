# DAML-RAG Framework

**Domain-Adaptive Meta-Learning RAG** - Production-Ready Framework for Vertical Domain AI Applications

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/Version-1.0.0-orange.svg)](CHANGELOG.md)

English | **[ç®€ä½“ä¸­æ–‡](README.md)**

> ğŸ“ **Combining GraphRAG, In-Context Learning, Multi-Agent Orchestration for Cost-Effective Vertical Domain AI**  
> ğŸš€ **Production-ready framework achieving token optimization and cost control through engineering best practices**

---

## ğŸ“– Overview

DAML-RAG is a production-ready framework that integrates proven techniquesâ€”GraphRAG hybrid retrieval, In-Context Learning, Teacher-Student collaboration, and MCP-based multi-agent orchestrationâ€”for building vertical domain AI applications with token savings and cost optimization.

**NOT a new theory, but an engineering best practice framework for practitioners.**

---

## ğŸ”¬ Academic Positioning

### What DAML-RAG IS âœ…

- **Engineering Framework**: Systematic integration of RAG [1], GraphRAG [2], ICL [3], Knowledge Graphs [4]
- **Production System**: Validated in BUILD_BODY fitness domain
- **Cost Optimization**: Teacher-student collaboration reducing costs
- **Vertical Domain Focus**: Specialized for knowledge-intensive domains

### What DAML-RAG is NOT âŒ

- âŒ **NOT a new ML/AI theory**: No novel algorithms or learning paradigms
- âŒ **NOT claiming universal superiority**: Designed for specific use cases
- âŒ **NOT automated domain adaptation**: Requires domain expertise for knowledge graph construction
- âŒ **NOT inference-time "meta-learning"**: Correctly termed "In-Context Learning" (v2.0 correction)

---

## ğŸ¯ Key Features

- ğŸ¯ **GraphRAG Hybrid Retrieval**: Vector + Graph + Rules three-tier architecture
- ğŸ§  **In-Context Learning** â­(v2.0 corrected): Few-Shot + Case-Based Reasoning for quality maintenance
- âš¡ **Teacher-Student Model**: DeepSeek (teacher) + Ollama (student) for cost reduction
- ğŸ”Œ **MCP Orchestration**: Standardized multi-agent collaboration via Model Context Protocol
- ğŸ›¡ï¸ **Quality Assurance**: Automatic quality monitoring and escalation
- ğŸ“Š **Production-Ready**: Complete monitoring, caching, fault tolerance

---

## ğŸ—ï¸ Core Architecture: Three-Tier Retrieval System

DAML-RAG's core innovation is the three-tier hybrid retrieval architecture, perfectly combining vector retrieval, knowledge graphs, and business rules:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query Input                          â”‚
â”‚      "Recommend leg muscle building exercises that           â”‚
â”‚       don't stress the knees"                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Vector Semantic Retrieval                          â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š Multiple Vector DB Support:                              â”‚
â”‚    â€¢ Qdrant (Recommended) - High-performance vector DB       â”‚
â”‚    â€¢ FAISS - Facebook AI Similarity Search                   â”‚
â”‚    â€¢ Milvus - Open-source vector database                    â”‚
â”‚    â€¢ Pinecone/Weaviate - Cloud vector services               â”‚
â”‚                                                              â”‚
â”‚  ğŸ” Semantic Similarity Matching:                            â”‚
â”‚    â€¢ Cosine Similarity                                       â”‚
â”‚    â€¢ HNSW Index Optimization (< 50ms response)               â”‚
â”‚    â€¢ Multi-language embedding model support                  â”‚
â”‚                                                              â”‚
â”‚  ğŸ¯ Core Functions:                                          â”‚
â”‚    â€¢ Understand user intent ("bulking" = "hypertrophy")      â”‚
â”‚    â€¢ Fuzzy matching (typos, synonym recognition)             â”‚
â”‚    â€¢ Fast candidate recall (Top 20-50)                       â”‚
â”‚    â€¢ Multi-modal retrieval (text, image, audio)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Knowledge Graph Reasoning                          â”‚
â”‚                                                              â”‚
â”‚  ğŸ•¸ï¸ Multiple Graph DB Support:                               â”‚
â”‚    â€¢ Neo4j (Recommended) - Professional graph database       â”‚
â”‚    â€¢ ArangoDB - Multi-model database                         â”‚
â”‚    â€¢ JanusGraph - Distributed graph database                 â”‚
â”‚    â€¢ Amazon Neptune - Cloud graph service                    â”‚
â”‚                                                              â”‚
â”‚  ğŸ”— Structured Relationship Reasoning:                       â”‚
â”‚    â€¢ Cypher Query Language (Neo4j)                           â”‚
â”‚    â€¢ Gremlin Graph Traversal Language                        â”‚
â”‚    â€¢ SPARQL Semantic Query                                   â”‚
â”‚    â€¢ Multi-hop reasoning capability (< 100ms)                â”‚
â”‚                                                              â”‚
â”‚  ğŸ¯ Core Functions:                                          â”‚
â”‚    â€¢ Precise filtering (based on 2,447+ entity nodes)        â”‚
â”‚    â€¢ Constraint validation ("no knee stress")                â”‚
â”‚    â€¢ Explainability (clear reasoning paths)                  â”‚
â”‚    â€¢ Multi-hop reasoning ("exerciseâ†’muscleâ†’goalâ†’constraint") â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Business Rule Filtering                           â”‚
â”‚                                                              â”‚
â”‚  ğŸ“‹ Domain Expert Rule Engine:                               â”‚
â”‚    â€¢ Safety rules (age, injury, recovery stage)              â”‚
â”‚    â€¢ Equipment rules (available devices, venue limits)       â”‚
â”‚    â€¢ Capacity rules (MRV, supercompensation, frequency)      â”‚
â”‚    â€¢ Personalization rules (user preferences, goal level)    â”‚
â”‚                                                              â”‚
â”‚  ğŸ›¡ï¸ Intelligent Validation System:                          â”‚
â”‚    â€¢ Dynamic rule loading (< 20ms)                           â”‚
â”‚    â€¢ Rule priority management                                â”‚
â”‚    â€¢ Rule conflict detection and resolution                  â”‚
â”‚    â€¢ Rule effectiveness evaluation and optimization          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š Final Results: 5 precise recommendations + reasoning +   â”‚
â”‚                    confidence scores                         â”‚
â”‚  ğŸ’¡ Token Optimization: Design target (not validated)        â”‚
â”‚  âš¡ Total Response Time: BUILD_BODY measured ~20s (laptop,   â”‚
â”‚                         not optimized)                       â”‚
â”‚  ğŸ¯ User Satisfaction: Design target (not validated)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Technology Stack Selection

DAML-RAG supports flexible technology stack selection, developers can choose based on preferences and needs:

```yaml
# Vector Database Options
Vector Databases:
  Qdrant:      â­â­â­â­â­ Recommended (high-performance, easy deployment)
  FAISS:       â­â­â­â­   (local deployment, fast)
  Milvus:      â­â­â­â­   (open-source, distributed)
  Pinecone:    â­â­â­     (cloud, managed service)
  Weaviate:    â­â­â­     (semantic search, GraphQL)

# Graph Database Options
Graph Databases:
  Neo4j:       â­â­â­â­â­ Recommended (professional graph DB)
  ArangoDB:    â­â­â­â­   (multi-model, flexible)
  JanusGraph:  â­â­â­     (distributed, big data)
  Neptune:     â­â­â­     (AWS integration)

# AI Model Selection
Large Models:
  DeepSeek:    â­â­â­â­â­ Teacher model (high-quality, Chinese-optimized)
  GPT-4:       â­â­â­â­   (strong general capability)
  Claude:      â­â­â­â­   (high security)
  Qwen:        â­â­â­â­   (open-source, Chinese)

Small Models:
  Ollama:      â­â­â­â­â­ Student model (local deployment, cost optimization)
  Llama:       â­â­â­â­   (open-source, good performance)
  Phi:         â­â­â­     (Microsoft, small and precise)
  Gemma:       â­â­â­     (Google, lightweight)
```

---

## ğŸ“¦ Module Structure

```
daml-rag-framework/
â”œâ”€â”€ daml-rag-core/              # ğŸ”§ Core Framework
â”‚   â”œâ”€â”€ interfaces/             # Abstract interface definitions
â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”œâ”€â”€ config/                 # Configuration management
â”‚   â””â”€â”€ utils/                  # Utility functions
â”œâ”€â”€ daml-rag-retrieval/         # ğŸ” Three-tier Retrieval Engine
â”‚   â”œâ”€â”€ vector/                 # Vector retrieval layer
â”‚   â”œâ”€â”€ knowledge/              # Knowledge graph layer
â”‚   â”œâ”€â”€ rules/                  # Rule filtering layer
â”‚   â””â”€â”€ cache/                  # Cache management
â”œâ”€â”€ daml-rag-orchestration/     # ğŸ¯ Task Orchestration Engine
â”œâ”€â”€ daml-rag-learning/          # ğŸ§  Inference-time Learning
â”œâ”€â”€ daml-rag-adapters/          # ğŸ”Œ Domain Adapters
â”œâ”€â”€ daml-rag-cli/               # ğŸš€ Command Line Tools
â””â”€â”€ examples/                   # ğŸ“š Example Projects
```

---

## ğŸš€ Quick Start

### Installation

```bash
pip install daml-rag-framework
```

### Create New Project

```bash
# Create fitness domain AI application
daml-rag init my-fitness-app --domain fitness

# Create healthcare domain AI application
daml-rag init my-medical-app --domain healthcare

# Create custom domain AI application
daml-rag init my-custom-app --template custom
```

### Basic Usage

```python
from daml_rag import DAMLRAGFramework, DAMLRAGConfig
from daml_rag_adapters.fitness import FitnessDomainAdapter

async def main():
    # Load configuration
    config = DAMLRAGConfig.from_file("config.yaml")
    
    # Create framework instance
    framework = DAMLRAGFramework(config)
    
    # Initialize domain adapter
    adapter = FitnessDomainAdapter(config.domain_config)
    await adapter.initialize()
    
    # Initialize framework
    await framework.initialize()
    
    # Process user query
    result = await framework.process_query("I want to create a muscle building plan")
    print(result.response)

if __name__ == "__main__":
    asyncio.run(main())
```

### Configuration Example

```yaml
# config.yaml
domain: fitness
debug: false

retrieval:
  vector_model: "BAAI/bge-base-zh-v1.5"
  top_k: 5
  similarity_threshold: 0.6
  cache_ttl: 300
  enable_kg: true
  enable_rules: true

orchestration:
  max_parallel_tasks: 10
  timeout_seconds: 30
  retry_attempts: 3
  enable_caching: true

learning:
  teacher_model: "deepseek"
  student_model: "ollama-qwen2.5"
  experience_threshold: 3.5
  feedback_weight: 0.8
  adaptive_threshold: 0.7

domain_config:
  knowledge_graph_path: "./data/knowledge_graph.db"
  mcp_servers:
    - name: "user-profile"
      command: "python"
      args: ["user-profile-stdio/server.py"]
    - name: "professional-coach"
      command: "python"
      args: ["professional-coach-stdio/server.py"]
```

---

## ğŸ“Š Current Status

**âš ï¸ Project Status: Production Preparation (Frontend Completion)**

### Actual Measured Data

| Metric | Current | Notes |
|--------|---------|-------|
| **Token/Query (Simple)** | 500-800 | DeepSeek + User Profile MCP |
| **Response Time** | **~20s** | âš ï¸ Not optimized, caching needed |
| **Project Stage** | Production Prep | Preparing for deployment |
| **MCP Tools Implemented** | 14/14 âœ… | All tools completed |
| **Docker Status** | In Use | Local deployment ready |
| **Frontend Status** | In Progress | Completing before deployment |

### Current Issues

**âš ï¸ Known Performance Issues:**

- **Slow Response**: ~20 seconds for simple queries
  - Cause: Unoptimized graph queries, no caching mechanism
  - Cause: Multiple sequential MCP calls, no parallelization
  - Status: Optimization planned for Phase 1
  
- **Frontend Completion**: In progress
  - Tools backend: âœ… Complete (14/14)
  - Frontend UI: ğŸš§ Completing
  - Docker deployment: âœ… Ready locally

- **Production Deployment**: Preparing
  - Local Docker: âœ… In use
  - Production deployment: ğŸš§ After frontend completion
  - Performance optimization: â³ Planned

### Known Limitations

**âš ï¸ IMPORTANT: Read [LIMITATIONS.md](LIMITATIONS.md) before use!**

Key limitations:

- **Hardware Requirements**: Minimum 16GB RAM, 32GB+ recommended
- **Response Time**: ~20 seconds (BUILD_BODY case on laptop, not optimized)
- **Scale Limits**: Performance degrades with >30K nodes on single machine
- **Deployment**: Distributed deployment recommended for production

See detailed analysis in [LIMITATIONS.md](LIMITATIONS.md).

### Design Targets (Not Yet Validated)

The following are **theoretical design goals**, not validated metrics:

- ğŸ¯ Token efficiency through GraphRAG hybrid retrieval
- ğŸ¯ Cost optimization via teacher-student collaboration  
- ğŸ¯ Quality improvement through structured knowledge
- ğŸ¯ Fast retrieval via vector + graph + rules

**Status**: Implementation in progress, benchmarks pending.

---

## ğŸ“š Documentation

### Essential Reading

- **[LIMITATIONS.md](LIMITATIONS.md)** âš ï¸ - Limitations and constraints (READ FIRST!)

### Theory

- [Theory Evolution](docs/theory/00-THEORY_EVOLUTION.md)
- [GraphRAG Hybrid Retrieval](docs/theory/01-GraphRAG-Hybrid-Retrieval.md)
- [In-Context Learning](docs/theory/02-In-Context-Learning.md)
- [Framework Overview](docs/theory/FRAMEWORK_OVERVIEW.md)

### Case Studies

- [BUILD_BODY Case Study](examples/BUILD_BODY_CASE_STUDY.md) (Coming soon) - Reference implementation

### Guides

- [Quick Start](docs/tutorials/quickstart.md) (Coming soon)
- [Architecture Design](docs/architecture/) (Coming soon)
- [API Reference](docs/api/) (Coming soon)
- [Deployment Guide](docs/tutorials/deployment.md) (Coming soon)

### References

- [Complete Bibliography](REFERENCES.md)
- [Citation](CITATION.cff)

---

## ğŸ“– Citation

If you use DAML-RAG in your research or project, please cite:

```bibtex
@software{daml_rag_2024,
  title={DAML-RAG: Domain-Adaptive Meta-Learning RAG Framework},
  author={è–›å°å· (Xue Xiaochuan)},
  year={2025},
  version={1.0.0},
  url={https://github.com/vivy1024/daml-rag-framework}
}
```

See [CITATION.cff](CITATION.cff) for detailed citation metadata.

**Copyright Â© 2025 è–›å°å· (Xue Xiaochuan). All rights reserved.**

---

## ğŸ¤ Contributing

Contributions are welcome! Please check:
- [CONTRIBUTING.md](CONTRIBUTING.md) (Coming soon)
- [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) (Coming soon)

---

## ğŸ“„ License

**Copyright Â© 2025 è–›å°å· (Xue Xiaochuan). All rights reserved.**

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built on theoretical and practical achievements from the BUILD_BODY v2.0 project.

**Standing on the shoulders of giants:**
- RAG: Lewis et al. (2020)
- GraphRAG: Microsoft Research (2025)
- In-Context Learning: Brown et al. (2020)
- Knowledge Graphs: Hogan et al. (2021)
- MCP: Anthropic (2025)

---

**Making AI Understand Professional Domains** ğŸš€

