# æ•°æ®æ¸…æ´—ä¸å¾®è°ƒæ¶æ„

**ç‰ˆæœ¬**: v1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2025-11-07  
**çŠ¶æ€**: ğŸ“‹ æ¶æ„è§„åˆ’é˜¶æ®µ(æœªå®æ–½)  

---

## âš ï¸ é‡è¦è¯´æ˜

æœ¬æ–‡æ¡£ä¸º**æ¶æ„è§„åˆ’æ–‡æ¡£**,**ä¸ç«‹å³å®æ–½**ã€‚

è¿™æ˜¯ä¸€ä¸ª**é€šç”¨æ¡†æ¶æ¨¡å—**,å¯åº”ç”¨äºä»»ä½• DAML-RAG åº”ç”¨ã€‚

**å®æ–½æ—¶æœº**:
- **æ•°æ®æ¸…æ´—**:æ¡†æ¶ç¨³å®šè¿è¡Œ 2 å‘¨åå¼€å§‹
- **å¾®è°ƒ**:æ•°æ®ç§¯ç´¯è¾¾æ ‡å(é¢„è®¡ 3-6 ä¸ªæœˆ)

---

## ğŸ“‹ æ–‡æ¡£ç›®çš„

æœ¬æ–‡æ¡£å®šä¹‰äº† DAML-RAG æ¡†æ¶åº”ç”¨çš„**é€šç”¨æ•°æ®æ¸…æ´—å’Œå¾®è°ƒæ¶æ„**ã€‚å®ƒæ˜¯**é¢†åŸŸæ— å…³çš„**,å¯ä»¥é€‚é…åˆ°:

- å¥èº«æ•™ç»ƒç³»ç»Ÿ
- å®¢æœèŠå¤©æœºå™¨äºº
- æ•™è‚²è¾…å¯¼ç³»ç»Ÿ
- åŒ»ç–—å’¨è¯¢ç³»ç»Ÿ
- ä»»ä½•é¢†åŸŸç‰¹å®šçš„ AI åŠ©æ‰‹

---

## 1. æ•°æ®æ¸…æ´—æµç¨‹

### 1.1 è§¦å‘æ¡ä»¶

```python
# è§¦å‘æ¡ä»¶(æ»¡è¶³ä»»ä¸€å³å¯)
triggers = {
    "weekly_schedule": "æ¯å‘¨æ—¥å‡Œæ™¨ 2:00 æ‰§è¡Œ",
    "conversation_threshold": "å¯¹è¯é‡è¾¾åˆ° 1000 æ¡"
}
```

### 1.2 æ¸…æ´—è§„åˆ™

#### è§„åˆ™ 1:è¿‡æ»¤ä½è´¨é‡å¯¹è¯

```sql
-- ä¿ç•™é«˜è´¨é‡å¯¹è¯(è¯„åˆ† >= 3.0)
SELECT *
FROM best_practices
WHERE reward >= 3.0
  AND created_at > DATE_SUB(NOW(), INTERVAL 7 DAY)
  AND user_id NOT LIKE 'test_%';  -- æ’é™¤æµ‹è¯•æ•°æ®
```

**åˆ¤æ–­ä¾æ®**:
- âœ… ç”¨æˆ·è¯„åˆ† â‰¥ 3.0
- âœ… 7 å¤©å†…çš„å¯¹è¯
- âœ… éæµ‹è¯•ç”¨æˆ·

#### è§„åˆ™ 2:å»é™¤é‡å¤æŸ¥è¯¢

```python
def deduplicate_by_similarity(conversations, threshold=0.95):
    """
    åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å»é‡
    
    Args:
        conversations: å¯¹è¯åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼(>= åˆ™åˆ¤å®šä¸ºé‡å¤)
    
    Returns:
        å»é‡åçš„å¯¹è¯åˆ—è¡¨
    """
    from sentence_transformers import SentenceTransformer
    import numpy as np
    
    model = SentenceTransformer('BAAI/bge-base-zh-v1.5')
    
    # ç¼–ç æ‰€æœ‰æŸ¥è¯¢
    queries = [conv['query'] for conv in conversations]
    embeddings = model.encode(queries, normalize_embeddings=True)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = np.dot(embeddings, embeddings.T)
    
    # å»é‡(ä¿ç•™è¯„åˆ†æœ€é«˜çš„)
    deduplicated = []
    used_indices = set()
    
    for i in range(len(conversations)):
        if i in used_indices:
            continue
        
        # æ‰¾åˆ°æ‰€æœ‰ä¸å½“å‰æŸ¥è¯¢ç›¸ä¼¼çš„ç´¢å¼•
        similar_indices = np.where(similarity_matrix[i] >= threshold)[0]
        
        # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„
        best_idx = max(similar_indices, key=lambda idx: conversations[idx]['reward'])
        deduplicated.append(conversations[best_idx])
        
        # æ ‡è®°å·²ä½¿ç”¨
        used_indices.update(similar_indices)
    
    return deduplicated
```

**å»é‡ç­–ç•¥**:
- ç›¸ä¼¼åº¦ **â‰¥ 0.95** â†’ åˆ¤å®šä¸ºé‡å¤
- ä¿ç•™è¯„åˆ†æœ€é«˜çš„å¯¹è¯
- è®°å½•å»é‡ç»Ÿè®¡ä¿¡æ¯

#### è§„åˆ™ 3:æ ‡æ³¨å·¥å…·é“¾

```python
def extract_tool_chain(tools_used_str):
    """
    ä» tools_used å­—æ®µæå–å·¥å…·é“¾
    
    Args:
        tools_used_str: "tool1,tool2,tool3" æˆ– JSONå­—ç¬¦ä¸²
    
    Returns:
        List[str]: å·¥å…·é“¾åˆ—è¡¨
    """
    import json
    
    try:
        # å°è¯•è§£æJSON
        if tools_used_str.startswith('['):
            return json.loads(tools_used_str)
        else:
            # é€—å·åˆ†éš”
            return [tool.strip() for tool in tools_used_str.split(',')]
    except:
        return []

def classify_domain(query):
    """
    åˆ†ç±»æŸ¥è¯¢é¢†åŸŸ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
    
    Returns:
        str: é¢†åŸŸæ ‡ç­¾
    """
    # é¢†åŸŸç‰¹å®šå…³é”®è¯(æ ¹æ®æ‚¨çš„åº”ç”¨å®šåˆ¶)
    keywords = {
        "domain_a": ["keyword1", "keyword2"],
        "domain_b": ["keyword3", "keyword4"],
        "domain_c": ["keyword5", "keyword6"],
        "general": []  # é€šç”¨/å…¶ä»–
    }
    
    for domain, kws in keywords.items():
        for kw in kws:
            if kw in query.lower():
                return domain
    
    return "general"
```

#### è§„åˆ™ 4:æ•°æ®éªŒè¯

```python
def validate_conversation(conv):
    """
    éªŒè¯å¯¹è¯æ•°æ®å®Œæ•´æ€§
    
    Returns:
        Tuple[bool, str]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åŸå› )
    """
    # å¿…å¡«å­—æ®µæ£€æŸ¥
    required_fields = ['query', 'response', 'tools_used', 'reward']
    for field in required_fields:
        if not conv.get(field):
            return False, f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}"
    
    # å·¥å…·é“¾æ£€æŸ¥
    if len(conv['tools_used']) == 0:
        return False, "å·¥å…·é“¾ä¸ºç©º"
    
    # å“åº”é•¿åº¦æ£€æŸ¥
    if len(conv['response']) < 50:
        return False, "å“åº”å†…å®¹è¿‡çŸ­"
    
    # è¯„åˆ†èŒƒå›´æ£€æŸ¥
    if not (1 <= conv['reward'] <= 5):
        return False, "è¯„åˆ†è¶…å‡ºèŒƒå›´ [1, 5]"
    
    return True, ""
```

### 1.3 æ¸…æ´—æµç¨‹å›¾

```
åŸå§‹å¯¹è¯æ•°æ®(æ•°æ®åº“: best_practices)
    â†“
[è§„åˆ™1] è¿‡æ»¤ä½è´¨é‡å¯¹è¯(reward < 3.0)
    â†“
[è§„åˆ™2] å»é™¤é‡å¤æŸ¥è¯¢(ç›¸ä¼¼åº¦ >= 0.95)
    â†“
[è§„åˆ™3] æ ‡æ³¨å·¥å…·é“¾å’Œé¢†åŸŸ
    â†“
[è§„åˆ™4] æ•°æ®éªŒè¯(å®Œæ•´æ€§æ£€æŸ¥)
    â†“
å­˜å‚¨åˆ°è®­ç»ƒæ•°æ®åº“(training_datasetè¡¨)
    â†“
ç”Ÿæˆæ¸…æ´—æŠ¥å‘Š(æ—¥å¿— + ç»Ÿè®¡ä¿¡æ¯)
```

### 1.4 å­˜å‚¨ç»“æ„

```sql
CREATE TABLE IF NOT EXISTS training_dataset (
    id INT PRIMARY KEY AUTO_INCREMENT,
    conversation_id VARCHAR(50) UNIQUE NOT NULL,  -- åŸå§‹å¯¹è¯ID
    user_id INT NOT NULL,
    query TEXT NOT NULL,
    response TEXT NOT NULL,
    tool_chain JSON NOT NULL,  -- ["tool1", "tool2", ...]
    domain VARCHAR(50) NOT NULL,  -- domain_a, domain_b, domain_c
    reward FLOAT NOT NULL,
    similarity_cluster INT DEFAULT NULL,  -- ç›¸ä¼¼åº¦èšç±»ID
    is_validated BOOLEAN DEFAULT FALSE,  -- æ˜¯å¦äººå·¥éªŒè¯è¿‡
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_domain (domain),
    INDEX idx_reward (reward),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

---

## 2. å¾®è°ƒè§¦å‘æ¡ä»¶

### 2.1 æ•°æ®é‡æ¡ä»¶

```python
def check_finetuning_readiness():
    """
    æ£€æŸ¥å¾®è°ƒå‡†å¤‡å°±ç»ªçŠ¶æ€
    
    Returns:
        Dict: å„é¡¹æŒ‡æ ‡çš„æ£€æŸ¥ç»“æœ
    """
    # 1. é«˜è´¨é‡å¯¹è¯æ•°é‡(è¯„åˆ† >= 4.5)
    high_quality_count = db.query("""
        SELECT COUNT(*) as cnt
        FROM training_dataset
        WHERE reward >= 4.5
    """).fetch_one()['cnt']
    
    # 2. åœºæ™¯è¦†ç›–åº¦
    coverage = db.query("""
        SELECT 
            domain,
            COUNT(*) as cnt,
            COUNT(*) * 100.0 / (SELECT COUNT(*) FROM training_dataset) as percentage
        FROM training_dataset
        GROUP BY domain
    """).fetch_all()
    
    coverage_dict = {row['domain']: row['percentage'] for row in coverage}
    
    # 3. Few-Shotæ£€ç´¢å‘½ä¸­ç‡
    few_shot_hit_rate = calculate_few_shot_hit_rate()  # éœ€è¦å®ç°
    
    return {
        "high_quality_count": {
            "value": high_quality_count,
            "threshold": 5000,
            "ready": high_quality_count >= 5000
        },
        "coverage": {
            "value": coverage_dict,
            "threshold": 0.20,  # æ¯ä¸ªåœºæ™¯è‡³å°‘20%
            "ready": all(v >= 20 for v in coverage_dict.values())
        },
        "few_shot_hit_rate": {
            "value": few_shot_hit_rate,
            "threshold": 0.60,
            "ready": few_shot_hit_rate < 0.60  # å‘½ä¸­ç‡ä½è¯´æ˜éœ€è¦å¾®è°ƒ
        }
    }
```

### 2.2 è§¦å‘é˜ˆå€¼

| æŒ‡æ ‡ | é˜ˆå€¼ | å½“å‰çŠ¶æ€(ç¤ºä¾‹) | æ˜¯å¦å°±ç»ª |
|-----|------|---------------|---------|
| **é«˜è´¨é‡å¯¹è¯æ•°** | â‰¥ 5000 æ¡ | 1200 æ¡ | âŒ |
| **åœºæ™¯è¦†ç›–åº¦** | æ¯ä¸ªåœºæ™¯ â‰¥ 20% | é¢†åŸŸA 35% / é¢†åŸŸB 30% / é¢†åŸŸC 15% / é¢†åŸŸD 20% | âŒ(é¢†åŸŸCä¸è¶³) |
| **Few-Shotå‘½ä¸­ç‡** | < 60% | 55% | âœ… |

**ç»¼åˆåˆ¤æ–­**:åªæœ‰å½“**æ‰€æœ‰æŒ‡æ ‡**éƒ½æ»¡è¶³æ—¶,æ‰å¯åŠ¨å¾®è°ƒã€‚

---

## 3. å¾®è°ƒæ¶æ„è®¾è®¡

### 3.1 åŸºç¡€æ¨¡å‹

```yaml
model:
  name: [æ‚¨é€‰æ‹©çš„åŸºç¡€æ¨¡å‹]
  parameters: [æ¨¡å‹å¤§å°]
  context_length: [ä¸Šä¸‹æ–‡çª—å£å¤§å°]
  license: [è®¸å¯è¯ç±»å‹]
  download: [æ¨¡å‹æ¥æº]
```

### 3.2 å¾®è°ƒæ–¹æ³•:LoRA

**LoRA (Low-Rank Adaptation)** å‚æ•°:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                # Rank(ç§©):æ§åˆ¶é€‚é…å™¨çš„å‚æ•°é‡
    lora_alpha=32,       # Alpha:ç¼©æ”¾å› å­(é€šå¸¸ä¸º r çš„ 2 å€)
    lora_dropout=0.05,   # Dropout:é˜²æ­¢è¿‡æ‹Ÿåˆ
    target_modules=[     # ç›®æ ‡æ¨¡å—:å¯¹å“ªäº›å±‚åº”ç”¨LoRA
        "q_proj",
        "v_proj",
        "k_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    bias="none",         # ä¸è®­ç»ƒbias
    task_type="CAUSAL_LM"  # ä»»åŠ¡ç±»å‹:å› æœè¯­è¨€æ¨¡å‹
)
```

**ä¸ºä»€ä¹ˆé€‰æ‹© LoRA?**
- âœ… å‚æ•°é‡å°(<1% åŸæ¨¡å‹å‚æ•°)
- âœ… è®­ç»ƒå¿«é€Ÿ(GPUæ—¶é—´å‡å°‘80%+)
- âœ… éƒ¨ç½²çµæ´»(å¯åˆ‡æ¢é€‚é…å™¨)
- âœ… é¿å…ç¾éš¾æ€§é—å¿˜

### 3.3 è®­ç»ƒæ•°æ®æ ¼å¼

```json
{
    "instruction": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹,åŸºäºç”¨æˆ·æŸ¥è¯¢å’Œå·¥å…·è°ƒç”¨ç»“æœ,ç”Ÿæˆä¸“ä¸šçš„å“åº”ã€‚",
    "input": "ç”¨æˆ·æŸ¥è¯¢: [æŸ¥è¯¢å†…å®¹]\n\nå·¥å…·è°ƒç”¨ç»“æœ:\n- tool1: {...}\n- tool2: {...}",
    "output": "æ ¹æ®æä¾›çš„ä¿¡æ¯...",
    "metadata": {
        "domain": "domain_a",
        "tool_chain": ["tool1", "tool2", "tool3"],
        "rating": 4.8
    }
}
```

### 3.4 è®­ç»ƒé…ç½®

```python
training_args = TrainingArguments(
    output_dir="./checkpoints",
    num_train_epochs=3,          # è®­ç»ƒè½®æ•°
    per_device_train_batch_size=4,  # æ‰¹æ¬¡å¤§å°(æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´)
    gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯ç§¯(æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡)
    learning_rate=3e-4,          # å­¦ä¹ ç‡
    lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è°ƒåº¦å™¨
    warmup_ratio=0.1,            # é¢„çƒ­æ¯”ä¾‹
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    fp16=True,                   # æ··åˆç²¾åº¦è®­ç»ƒ(èŠ‚çœæ˜¾å­˜)
    gradient_checkpointing=True, # æ¢¯åº¦æ£€æŸ¥ç‚¹(è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜)
    max_grad_norm=1.0,          # æ¢¯åº¦è£å‰ª
    save_total_limit=3,         # åªä¿ç•™æœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss"
)
```

### 3.5 GPU èµ„æºéœ€æ±‚

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒæ—¶é—´ | æˆæœ¬(AWS/Azure) |
|-----|---------|---------|-----------------|
| **æœ€ä½é…ç½®** | 24GB(RTX 4090) | 12-18 å°æ—¶ | $50-80 |
| **æ¨èé…ç½®** | 40GB(A100) | 6-8 å°æ—¶ | $100-150 |
| **é«˜æ€§èƒ½é…ç½®** | 80GB(A100 80GB) | 3-4 å°æ—¶ | $200-300 |

---

## 4. éƒ¨ç½²ç­–ç•¥

### 4.1 å®Œæ•´éƒ¨ç½²æµç¨‹

```
æ•°æ®ç§¯ç´¯è¾¾æ ‡
    â†“
ã€é˜¶æ®µ1ã€‘è®­ç»ƒ LoRA é€‚é…å™¨(GPUæœåŠ¡å™¨)
    â”œâ”€ æ•°æ®å‡†å¤‡(è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›† = 8:1:1)
    â”œâ”€ è®­ç»ƒ(3-5 epochs)
    â”œâ”€ æ¨¡å‹åˆå¹¶(BaseModel + LoRA Adapter)
    â””â”€ ç¦»çº¿è¯„ä¼°(æµ‹è¯•é›†)
    â†“
ã€é˜¶æ®µ2ã€‘ç¦»çº¿éªŒè¯
    â”œâ”€ äººå·¥è¯„ä¼°(æŠ½æ ·100æ¡)
    â”œâ”€ è‡ªåŠ¨è¯„ä¼°(å·¥å…·é“¾å‡†ç¡®ç‡ã€å“åº”è´¨é‡)
    â””â”€ é€šè¿‡æ¡ä»¶:è¯„åˆ† >= åŸºçº¿æ¨¡å‹
    â†“
ã€é˜¶æ®µ3ã€‘A/B æµ‹è¯•(7å¤©)
    â”œâ”€ 50% æµé‡ â†’ å¾®è°ƒæ¨¡å‹
    â”œâ”€ 50% æµé‡ â†’ åŸºçº¿æ¨¡å‹
    â”œâ”€ ç›‘æ§æŒ‡æ ‡(ç”¨æˆ·æ»¡æ„åº¦ã€å“åº”æ—¶é—´ã€å·¥å…·é“¾å‡†ç¡®ç‡)
    â””â”€ å†³ç­–:ç»§ç»­ or å›æ»š
    â†“
ã€é˜¶æ®µ4ã€‘ç°åº¦å‘å¸ƒ
    â”œâ”€ 70% æµé‡(3å¤©)
    â”œâ”€ 90% æµé‡(3å¤©)
    â””â”€ 100% æµé‡(å…¨é‡)
    â†“
ã€é˜¶æ®µ5ã€‘å…¨é‡æ›¿æ¢
    â”œâ”€ æ›´æ–°éƒ¨ç½²æ¨¡å‹
    â”œâ”€ ä¿ç•™åŸæ¨¡å‹ä½œä¸ºé™çº§æ–¹æ¡ˆ
    â””â”€ æŒç»­ç›‘æ§ 30 å¤©
```

### 4.2 é™çº§ç­–ç•¥

```python
class ModelManager:
    def __init__(self):
        self.current_model = "finetuned-model"
        self.baseline_model = "base-model"
        self.degraded = False
    
    def check_health(self):
        """æ£€æŸ¥æ¨¡å‹å¥åº·åº¦"""
        metrics = get_recent_metrics(hours=1)
        
        # é™çº§æ¡ä»¶
        if (metrics['user_satisfaction'] < 3.8 or
            metrics['error_rate'] > 0.05 or
            metrics['p95_latency'] > 5.0):
            self.degrade()
    
    def degrade(self):
        """é™çº§åˆ°åŸºçº¿æ¨¡å‹"""
        logger.warning("ğŸš¨ æ£€æµ‹åˆ°å¼‚å¸¸,é™çº§åˆ°åŸºçº¿æ¨¡å‹")
        self.current_model = self.baseline_model
        self.degraded = True
        send_alert("æ¨¡å‹é™çº§é€šçŸ¥")
```

---

## 5. è¯„ä¼°æŒ‡æ ‡

### 5.1 æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | æŒ‡æ ‡åç§° | åŸºçº¿ | ç›®æ ‡ | æƒé‡ | è®¡ç®—æ–¹å¼ |
|---------|---------|------|-----|------|---------|
| **ç”¨æˆ·ä½“éªŒ** | ç”¨æˆ·æ»¡æ„åº¦(å¹³å‡è¯„åˆ†) | 4.2 | 4.5+ | â­â­â­â­â­ | AVG(user_rating) |
| **å‡†ç¡®æ€§** | å·¥å…·é“¾å‡†ç¡®ç‡ | 75% | 85%+ | â­â­â­â­â­ | Correct / Total |
| **æ•ˆç‡** | å“åº”æ—¶é—´ (P95) | 3.5s | < 4.0s | â­â­â­â­ | Percentile(95) |
| **æˆæœ¬** | APIæˆæœ¬é™ä½ | åŸºå‡† | 30%+ | â­â­â­â­ | (Baseline - Current) / Baseline |
| **æ™ºèƒ½æ€§** | Few-Shotå‘½ä¸­ç‡ | 55% | 70%+ | â­â­â­ | Hits / Total |

---

## 6. é£é™©è¯„ä¼°ä¸ç¼“è§£

### 6.1 é£é™©æ¸…å•

| é£é™©ç±»å‹ | å…·ä½“æè¿° | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|---------|---------|------|-----|---------|
| **æ•°æ®è´¨é‡** | æ¸…æ´—åæ•°æ®ä»æœ‰å™ªå£° | ä¸­ | é«˜ | äººå·¥æŠ½æŸ¥100æ¡ + ä¸¥æ ¼éªŒè¯è§„åˆ™ |
| **è¿‡æ‹Ÿåˆ** | æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½,æµ‹è¯•é›†å·® | ä¸­ | ä¸­ | Early stopping + éªŒè¯é›†ç›‘æ§ + Dropout |
| **æ€§èƒ½é™ä½** | å¾®è°ƒåå“åº”æ—¶é—´å˜é•¿ | ä½ | é«˜ | æ¨¡å‹é‡åŒ– + æ¨ç†ä¼˜åŒ– + A/Bæµ‹è¯• |
| **åœºæ™¯è¦†ç›–ä¸è¶³** | æŸäº›åœºæ™¯æ•°æ®è¿‡å°‘ | ä¸­ | ä¸­ | ä¸»åŠ¨ç”Ÿæˆè¡¥å……æ•°æ® + è¿ç§»å­¦ä¹  |
| **ç¾éš¾æ€§é—å¿˜** | å¾®è°ƒåé€šç”¨èƒ½åŠ›ä¸‹é™ | ä½ | ä¸­ | LoRAæ–¹æ³• + ä¿ç•™åŸºç¡€ä»»åŠ¡æ•°æ® |
| **GPUèµ„æºä¸è¶³** | è®­ç»ƒæ—¶GPUä¸å¯ç”¨/æˆæœ¬è¶…é¢„ç®— | ä½ | ä¸­ | ç§Ÿç”¨äº‘GPU(AWS/Azure) + è®­ç»ƒé˜Ÿåˆ— |
| **æ¨¡å‹æ³„æ¼** | å¾®è°ƒæ¨¡å‹è¢«ç›—ç”¨ | ä½ | é«˜ | è®¿é—®æ§åˆ¶ + æ¨¡å‹åŠ å¯† + å®¡è®¡æ—¥å¿— |

---

## 7. å®æ–½æ—¶é—´è¡¨

### 7.1 é‡Œç¨‹ç¢‘

| é˜¶æ®µ | æ—¶é—´ç‚¹ | å…³é”®ä»»åŠ¡ | äº¤ä»˜ç‰© |
|-----|-------|---------|--------|
| **P0** | æ¡†æ¶ç¨³å®šå +2å‘¨ | å®æ–½æ•°æ®æ¸…æ´—æµç¨‹ | æ¸…æ´—è„šæœ¬ + é¦–æ¬¡æ¸…æ´—æŠ¥å‘Š |
| **P1** | +1ä¸ªæœˆ | æŒç»­æ•°æ®ç§¯ç´¯ | æ¯å‘¨æ¸…æ´—æŠ¥å‘Š |
| **P2** | +3ä¸ªæœˆ | æ£€æŸ¥å¾®è°ƒå°±ç»ªæ¡ä»¶ | å°±ç»ªè¯„ä¼°æŠ¥å‘Š |
| **P3** | +3.5ä¸ªæœˆ | è®­ç»ƒLoRAé€‚é…å™¨ | å¾®è°ƒæ¨¡å‹ + è¯„ä¼°æŠ¥å‘Š |
| **P4** | +4ä¸ªæœˆ | A/Bæµ‹è¯• | A/Bæµ‹è¯•æŠ¥å‘Š + å†³ç­– |
| **P5** | +4.5ä¸ªæœˆ | ç°åº¦å‘å¸ƒ | ç°åº¦å‘å¸ƒæŠ¥å‘Š |
| **P6** | +5ä¸ªæœˆ | å…¨é‡æ›¿æ¢ | ä¸Šçº¿å…¬å‘Š + ç›‘æ§çœ‹æ¿ |

---

## 8. å‚è€ƒèµ„æº

### 8.1 è®ºæ–‡

1. **LoRA**: ["LoRA: Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2106.09685) (Hu et al., 2021)
2. **PEFT**: ["Parameter-Efficient Fine-Tuning of Large-Scale Pre-Trained Language Models"](https://arxiv.org/abs/2110.04366) (Liu et al., 2021)
3. **Instruction Tuning**: ["Finetuned Language Models Are Zero-Shot Learners"](https://arxiv.org/abs/2109.01652) (Wei et al., 2021)

### 8.2 å·¥å…·

- **Hugging Face Transformers**: https://huggingface.co/docs/transformers
- **PEFTåº“**: https://github.com/huggingface/peft
- **Sentence Transformers**: https://www.sbert.net/
- **Weights & Biases (è®­ç»ƒç›‘æ§)**: https://wandb.ai/

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **æ¡†æ¶å¤šæ ·æ€§æ¢ç´¢ç­–ç•¥**: [æ¡†æ¶å¤šæ ·æ€§æ¢ç´¢ç­–ç•¥.md](./æ¡†æ¶å¤šæ ·æ€§æ¢ç´¢ç­–ç•¥.md)
- **English Version**: [data-cleaning-and-finetuning.md](./data-cleaning-and-finetuning.md)

---

**ç»´æŠ¤è€…**: DAML-RAG Framework Team  
**æœ€åå®¡æŸ¥**: 2025-11-07

<div align="center">
<strong>ğŸ“Š æ•°æ®é©±åŠ¨ Â· ğŸ¯ ç²¾å‡†ä¼˜åŒ– Â· ğŸš€ æŒç»­æ¼”è¿›</strong>
</div>

---

## ğŸ“– åº”ç”¨åˆ°æ‚¨çš„é¢†åŸŸ

è¦å°†æ­¤é€šç”¨æ¶æ„åº”ç”¨åˆ°æ‚¨çš„é¢†åŸŸ:
1. åœ¨ `classify_domain()` ä¸­å®šåˆ¶é¢†åŸŸç‰¹å®šå…³é”®è¯
2. å®šä¹‰æ‚¨çš„é¢†åŸŸç‰¹å®šè¯„ä¼°æŒ‡æ ‡
3. æ ¹æ®æ‚¨çš„æ•°æ®è§„æ¨¡è°ƒæ•´è§¦å‘é˜ˆå€¼
4. æ ¹æ®æ‚¨çš„ç”¨ä¾‹é€‰æ‹©åˆé€‚çš„åŸºç¡€æ¨¡å‹

### å®æ–½å»ºè®®
1. **æ•°æ®å…ˆè¡Œ**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡æ•°æ®
2. **æ¸è¿›ä¼˜åŒ–**ï¼šä»ç®€å•æ–¹æ¡ˆå¼€å§‹
3. **ä¸¥æ ¼æµ‹è¯•**ï¼šæ¯ä¸ªé˜¶æ®µå……åˆ†éªŒè¯
4. **ä¿ç•™é™çº§**ï¼šå§‹ç»ˆæœ‰å›é€€æ–¹æ¡ˆ

