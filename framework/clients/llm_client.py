# -*- coding: utf-8 -*-
"""
LLMè°ƒç”¨æ¨¡å—

æ”¯æŒå¤šä¸ªLLMæä¾›å•†:
- DeepSeek (teacheræ¨¡å‹ï¼Œç»æµå®æƒ )
- Ollama (studentæ¨¡å‹ï¼Œæœ¬åœ°éƒ¨ç½²)
- Moonshot Kimi (å¤‡ç”¨ï¼Œé•¿æ–‡æœ¬)

ä½œè€…: BUILD_BODY Team
ç‰ˆæœ¬: v1.0.0
"""

import os
import logging
import json
from typing import List, Dict, Any, Optional, AsyncIterator
import httpx

logger = logging.getLogger(__name__)


class LLMConfig:
    """LLMé…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–"""

    # DeepSeek (teacheræ¨¡å‹)
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
    DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")
    DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

    # Ollama (studentæ¨¡å‹)
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen3:8b")

    # Moonshot (å¤‡ç”¨)
    MOONSHOT_API_KEY = os.getenv("MOONSHOT_API_KEY", "")
    MOONSHOT_BASE_URL = os.getenv("MOONSHOT_BASE_URL", "https://api.moonshot.cn/v1")
    MOONSHOT_MODEL = os.getenv("MOONSHOT_MODEL", "moonshot-v1-32k")

    # é€šä¹‰åƒé—® (å¤‡ç”¨)
    QWEN_API_KEY = os.getenv("QWEN_API_KEY", "")
    QWEN_BASE_URL = os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    QWEN_MODEL = os.getenv("QWEN_MODEL", "qwen-turbo")

    # é€šç”¨é…ç½®
    TIMEOUT = float(os.getenv("LLM_TIMEOUT", "60.0"))
    MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "2000"))
    TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))

    @classmethod
    def validate(cls):
        """éªŒè¯å¿…éœ€çš„APIå¯†é’¥æ˜¯å¦å·²é…ç½®"""
        if not cls.DEEPSEEK_API_KEY:
            logger.warning("DEEPSEEK_API_KEYæœªé…ç½®ï¼ŒDeepSeekåŠŸèƒ½å°†ä¸å¯ç”¨")
        if not cls.MOONSHOT_API_KEY:
            logger.warning("MOONSHOT_API_KEYæœªé…ç½®ï¼ŒMoonshotåŠŸèƒ½å°†ä¸å¯ç”¨")
        if not cls.QWEN_API_KEY:
            logger.warning("QWEN_API_KEYæœªé…ç½®ï¼Œé€šä¹‰åƒé—®åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    @classmethod
    def validate_dependencies(cls):
        """
        éªŒè¯æ‰€æœ‰å¿…éœ€çš„ä¾èµ–é¡¹æ˜¯å¦å­˜åœ¨
        
        Raises:
            ImportError: å¦‚æœç¼ºå°‘å¿…éœ€çš„ä¾èµ–æ¨¡å—
        """
        required_modules = ["httpx", "json", "asyncio", "logging"]
        missing_modules = []
        
        for module in required_modules:
            try:
                __import__(module)
            except ImportError:
                missing_modules.append(module)
                logger.error(f"ç¼ºå°‘å¿…éœ€æ¨¡å—: {module}")
        
        if missing_modules:
            raise ImportError(
                f"ç¼ºå°‘å¿…éœ€çš„ä¾èµ–æ¨¡å—: {', '.join(missing_modules)}ã€‚"
                f"è¯·è¿è¡Œ: pip install {' '.join(missing_modules)}"
            )


async def call_deepseek(
    query: str,
    few_shot_examples: List[Dict[str, Any]],
    tool_results: Dict[str, Any],
    system_prompt: str = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¥èº«æ•™ç»ƒï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·æ¡£æ¡ˆæä¾›ä¸ªæ€§åŒ–çš„è®­ç»ƒå»ºè®®ã€‚",
    max_tokens: int = LLMConfig.MAX_TOKENS
) -> str:
    """
    è°ƒç”¨DeepSeek API (teacheræ¨¡å‹)

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        few_shot_examples: Few-Shotç¤ºä¾‹åˆ—è¡¨
        tool_results: å·¥å…·è°ƒç”¨ç»“æœ
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

    Returns:
        str: AIå›ç­”
    """
    # æ£€æŸ¥APIå¯†é’¥
    if not LLMConfig.DEEPSEEK_API_KEY:
        raise ValueError("DEEPSEEK_API_KEYæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")

    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # æ·»åŠ Few-Shotç¤ºä¾‹
        for example in few_shot_examples:
            messages.append({
                "role": "user",
                "content": example.get("query", "")
            })
            messages.append({
                "role": "assistant",
                "content": example.get("response", "")
            })

        # æ·»åŠ å·¥å…·ç»“æœåˆ°ä¸Šä¸‹æ–‡
        tool_context = _format_tool_results(tool_results)
        if tool_context:
            messages.append({
                "role": "system",
                "content": f"å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_context}"
            })

        # æ·»åŠ å½“å‰æŸ¥è¯¢
        messages.append({
            "role": "user",
            "content": query
        })

        # è°ƒç”¨DeepSeek API
        async with httpx.AsyncClient(timeout=LLMConfig.TIMEOUT) as client:
            response = await client.post(
                f"{LLMConfig.DEEPSEEK_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLMConfig.DEEPSEEK_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": LLMConfig.DEEPSEEK_MODEL,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": LLMConfig.TEMPERATURE
                }
            )

            response.raise_for_status()
            result = response.json()

            answer = result["choices"][0]["message"]["content"]

            logger.info(
                f"DeepSeekè°ƒç”¨æˆåŠŸ: "
                f"tokens={result.get('usage', {}).get('total_tokens', 0)}, "
                f"length={len(answer)}"
            )

            return answer

    except httpx.HTTPError as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"DeepSeek HTTPé”™è¯¯: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {LLMConfig.DEEPSEEK_MODEL}\n"
            f"  - æŸ¥è¯¢é•¿åº¦: {len(query)}\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - å·¥å…·ç»“æœæ•°: {len(tool_results) if tool_results else 0}\n"
            f"  - Max Tokens: {max_tokens}\n"
            f"  - Temperature: {LLMConfig.TEMPERATURE}",
            exc_info=True
        )
        # è¿”å›é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return get_fallback_response(
            query=query,
            tool_results=tool_results,
            reason=f"DeepSeek HTTPé”™è¯¯: {str(e)}"
        )
    except Exception as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"DeepSeekè°ƒç”¨å¤±è´¥: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {LLMConfig.DEEPSEEK_MODEL}\n"
            f"  - æŸ¥è¯¢: {query[:100]}...\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}",
            exc_info=True
        )
        # è¿”å›é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return get_fallback_response(
            query=query,
            tool_results=tool_results,
            reason=f"DeepSeekè°ƒç”¨å¤±è´¥: {str(e)}"
        )


async def call_ollama(
    query: str,
    few_shot_examples: List[Dict[str, Any]],
    tool_results: Dict[str, Any],
    system_prompt: str = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¥èº«æ•™ç»ƒï¼Œæ“…é•¿æ ¹æ®ç”¨æˆ·æ¡£æ¡ˆæä¾›ä¸ªæ€§åŒ–çš„è®­ç»ƒå»ºè®®ã€‚",
    model: str = LLMConfig.OLLAMA_MODEL
) -> str:
    """
    è°ƒç”¨Ollama API (studentæ¨¡å‹)

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        few_shot_examples: Few-Shotç¤ºä¾‹åˆ—è¡¨
        tool_results: å·¥å…·è°ƒç”¨ç»“æœ
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        model: Ollamaæ¨¡å‹åç§°

    Returns:
        str: AIå›ç­”
    """
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # æ·»åŠ Few-Shotç¤ºä¾‹
        for example in few_shot_examples:
            messages.append({
                "role": "user",
                "content": example.get("query", "")
            })
            messages.append({
                "role": "assistant",
                "content": example.get("response", "")
            })

        # æ·»åŠ å·¥å…·ç»“æœåˆ°ä¸Šä¸‹æ–‡
        tool_context = _format_tool_results(tool_results)
        if tool_context:
            messages.append({
                "role": "system",
                "content": f"å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_context}"
            })

        # æ·»åŠ å½“å‰æŸ¥è¯¢
        messages.append({
            "role": "user",
            "content": query
        })

        # è°ƒç”¨Ollama API
        async with httpx.AsyncClient(timeout=LLMConfig.TIMEOUT) as client:
            response = await client.post(
                f"{LLMConfig.OLLAMA_BASE_URL}/api/chat",
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False
                }
            )

            response.raise_for_status()
            result = response.json()

            answer = result["message"]["content"]

            logger.info(
                f"Ollamaè°ƒç”¨æˆåŠŸ: model={model}, length={len(answer)}"
            )

            return answer

    except httpx.HTTPError as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"Ollama HTTPé”™è¯¯: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {model}\n"
            f"  - åŸºç¡€URL: {LLMConfig.OLLAMA_BASE_URL}\n"
            f"  - æŸ¥è¯¢é•¿åº¦: {len(query)}\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - å·¥å…·ç»“æœæ•°: {len(tool_results) if tool_results else 0}",
            exc_info=True
        )
        # è¿”å›é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return get_fallback_response(
            query=query,
            tool_results=tool_results,
            reason=f"Ollama HTTPé”™è¯¯: {str(e)}"
        )
    except Exception as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"Ollamaè°ƒç”¨å¤±è´¥: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {model}\n"
            f"  - æŸ¥è¯¢: {query[:100]}...\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}",
            exc_info=True
        )
        # è¿”å›é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return get_fallback_response(
            query=query,
            tool_results=tool_results,
            reason=f"Ollamaè°ƒç”¨å¤±è´¥: {str(e)}"
        )


async def call_moonshot(
    query: str,
    few_shot_examples: List[Dict[str, Any]],
    tool_results: Dict[str, Any],
    system_prompt: str = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¥èº«æ•™ç»ƒã€‚"
) -> str:
    """
    è°ƒç”¨Moonshot API (å¤‡ç”¨ï¼Œé€‚åˆé•¿æ–‡æœ¬)

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        few_shot_examples: Few-Shotç¤ºä¾‹åˆ—è¡¨
        tool_results: å·¥å…·è°ƒç”¨ç»“æœ
        system_prompt: ç³»ç»Ÿæç¤ºè¯

    Returns:
        str: AIå›ç­”
    """
    # æ£€æŸ¥APIå¯†é’¥
    if not LLMConfig.MOONSHOT_API_KEY:
        raise ValueError("MOONSHOT_API_KEYæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")

    try:
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # æ·»åŠ Few-Shotç¤ºä¾‹
        for example in few_shot_examples:
            messages.append({
                "role": "user",
                "content": example.get("query", "")
            })
            messages.append({
                "role": "assistant",
                "content": example.get("response", "")
            })

        # æ·»åŠ å·¥å…·ç»“æœ
        tool_context = _format_tool_results(tool_results)
        if tool_context:
            messages.append({
                "role": "system",
                "content": f"å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_context}"
            })

        messages.append({
            "role": "user",
            "content": query
        })

        async with httpx.AsyncClient(timeout=LLMConfig.TIMEOUT) as client:
            response = await client.post(
                f"{LLMConfig.MOONSHOT_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLMConfig.MOONSHOT_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": LLMConfig.MOONSHOT_MODEL,
                    "messages": messages,
                    "max_tokens": LLMConfig.MAX_TOKENS,
                    "temperature": LLMConfig.TEMPERATURE
                }
            )

            response.raise_for_status()
            result = response.json()

            return result["choices"][0]["message"]["content"]

    except Exception as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"Moonshotè°ƒç”¨å¤±è´¥: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {LLMConfig.MOONSHOT_MODEL}\n"
            f"  - æŸ¥è¯¢: {query[:100]}...\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}\n"
            f"  - Max Tokens: {LLMConfig.MAX_TOKENS}",
            exc_info=True
        )
        # è¿”å›é™çº§å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return get_fallback_response(
            query=query,
            tool_results=tool_results,
            reason=f"Moonshotè°ƒç”¨å¤±è´¥: {str(e)}"
        )


def _format_tool_results(tool_results: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–å·¥å…·ç»“æœä¸ºå¯è¯»æ–‡æœ¬

    Args:
        tool_results: å·¥å…·è°ƒç”¨ç»“æœå­—å…¸

    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not tool_results:
        return ""

    lines = []
    for tool_name, result in tool_results.items():
        lines.append(f"ã€{tool_name}ã€‘")

        if isinstance(result, dict):
            for key, value in result.items():
                lines.append(f"  - {key}: {value}")
        elif isinstance(result, list):
            for item in result:
                lines.append(f"  - {item}")
        else:
            lines.append(f"  {result}")

    return "\n".join(lines)


def get_fallback_response(
    query: str,
    tool_results: Optional[Dict[str, Any]] = None,
    reason: str = "LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
) -> str:
    """
    è·å–é™çº§å“åº”ï¼ˆåŸºäºæŸ¥è¯¢å’Œå·¥å…·ç»“æœç”Ÿæˆæœ‰æ„ä¹‰çš„é»˜è®¤å“åº”ï¼‰
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        tool_results: å·¥å…·è°ƒç”¨ç»“æœï¼ˆå¯é€‰ï¼‰
        reason: é™çº§åŸå› 
    
    Returns:
        str: æœ‰æ„ä¹‰çš„é™çº§å“åº”æ–‡æœ¬
    """
    logger.warning(f"ä½¿ç”¨é™çº§å“åº”: {reason}")
    
    # åŸºç¡€å“åº”æ¨¡æ¿
    response_parts = [
        f"æŠ±æ­‰ï¼ŒAIåˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼ˆ{reason}ï¼‰ã€‚",
        "",
        f"ğŸ“ æ‚¨çš„æŸ¥è¯¢ï¼š{query}",
        ""
    ]
    
    # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œæä¾›ç»“æ„åŒ–ä¿¡æ¯
    if tool_results:
        response_parts.append("âœ… å·²å®Œæˆä»¥ä¸‹æ•°æ®åˆ†æï¼š")
        response_parts.append("")
        
        # æå–å…³é”®ä¿¡æ¯
        if "step1_user_profile" in tool_results:
            profile = tool_results["step1_user_profile"]
            if profile and isinstance(profile, dict):
                response_parts.append("ğŸ‘¤ **ç”¨æˆ·æ¡£æ¡ˆ**ï¼š")
                if "age" in profile:
                    response_parts.append(f"  - å¹´é¾„ï¼š{profile['age']}å²")
                if "primary_goal" in profile:
                    response_parts.append(f"  - ç›®æ ‡ï¼š{profile['primary_goal']}")
                if "fitness_level" in profile:
                    response_parts.append(f"  - æ°´å¹³ï¼š{profile['fitness_level']}")
                response_parts.append("")
        
        if "step4_complexity" in tool_results:
            complexity = tool_results["step4_complexity"]
            if complexity and isinstance(complexity, dict):
                is_complex = complexity.get("is_complex", False)
                response_parts.append(f"ğŸ” **æŸ¥è¯¢åˆ†æ**ï¼š{'å¤æ‚' if is_complex else 'ç®€å•'}æŸ¥è¯¢")
                response_parts.append("")
        
        if "step8_retrieval_results" in tool_results:
            retrieval = tool_results["step8_retrieval_results"]
            if retrieval and isinstance(retrieval, dict):
                count = retrieval.get("count", 0)
                response_parts.append(f"ğŸ“Š **æ£€ç´¢ç»“æœ**ï¼šæ‰¾åˆ° {count} ä¸ªç›¸å…³æ¨è")
                response_parts.append("")
    
    # æ·»åŠ å»ºè®®
    response_parts.extend([
        "ğŸ’¡ **å»ºè®®**ï¼š",
        "  - è¯·ç¨åé‡è¯•è·å–AIåˆ†æ",
        "  - æˆ–è”ç³»å®¢æœè·å–äººå·¥æŒ‡å¯¼",
        "  - æ‚¨ä¹Ÿå¯ä»¥æŸ¥çœ‹ä¸Šè¿°æ•°æ®è‡ªè¡Œåˆ†æ",
        "",
        "æ„Ÿè°¢æ‚¨çš„ç†è§£ï¼"
    ])
    
    return "\n".join(response_parts)


async def stream_deepseek(
    query: str,
    few_shot_examples: List[Dict[str, Any]],
    tool_results: Dict[str, Any],
    system_prompt: str = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¥èº«æ•™ç»ƒã€‚"
) -> AsyncIterator[str]:
    """
    æµå¼è°ƒç”¨DeepSeek API

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        few_shot_examples: Few-Shotç¤ºä¾‹åˆ—è¡¨
        tool_results: å·¥å…·è°ƒç”¨ç»“æœ
        system_prompt: ç³»ç»Ÿæç¤ºè¯

    Yields:
        str: æµå¼è¿”å›çš„æ–‡æœ¬ç‰‡æ®µ
    """
    # æ£€æŸ¥APIå¯†é’¥
    if not LLMConfig.DEEPSEEK_API_KEY:
        raise ValueError("DEEPSEEK_API_KEYæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")

    try:
        # æ„å»ºæ¶ˆæ¯ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        messages = [{"role": "system", "content": system_prompt}]

        for example in few_shot_examples:
            messages.append({"role": "user", "content": example.get("query", "")})
            messages.append({"role": "assistant", "content": example.get("response", "")})

        tool_context = _format_tool_results(tool_results)
        if tool_context:
            messages.append({"role": "system", "content": f"å·¥å…·è°ƒç”¨ç»“æœ:\n{tool_context}"})

        messages.append({"role": "user", "content": query})

        # æµå¼è°ƒç”¨
        async with httpx.AsyncClient(timeout=LLMConfig.TIMEOUT) as client:
            async with client.stream(
                "POST",
                f"{LLMConfig.DEEPSEEK_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLMConfig.DEEPSEEK_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": LLMConfig.DEEPSEEK_MODEL,
                    "messages": messages,
                    "max_tokens": LLMConfig.MAX_TOKENS,
                    "temperature": LLMConfig.TEMPERATURE,
                    "stream": True
                }
            ) as response:
                response.raise_for_status()

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]  # å»æ‰ "data: " å‰ç¼€

                        if data == "[DONE]":
                            break

                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0]["delta"]

                            if "content" in delta:
                                yield delta["content"]
                        except Exception as e:
                            logger.warning(f"è§£ææµå¼å“åº”å¤±è´¥: {e}")
                            continue

    except Exception as e:
        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡
        logger.error(
            f"DeepSeekæµå¼è°ƒç”¨å¤±è´¥: {e}\n"
            f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n"
            f"  - æ¨¡å‹: {LLMConfig.DEEPSEEK_MODEL}\n"
            f"  - æŸ¥è¯¢: {query[:100]}...\n"
            f"  - Few-Shotç¤ºä¾‹æ•°: {len(few_shot_examples)}\n"
            f"  - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}\n"
            f"  - æµå¼æ¨¡å¼: True",
            exc_info=True
        )
        raise
