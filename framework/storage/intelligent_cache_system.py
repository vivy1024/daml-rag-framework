# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ v3.0 - åŸºäºDAGæ¨¡æ¿çš„æ™ºèƒ½ç¼“å­˜ç®¡ç†

ä¸“ä¸º23ä¸ªä¸“ä¸šå¥èº«å·¥å…·è®¾è®¡çš„æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒå¤šçº§ç¼“å­˜ã€TTLç®¡ç†ã€åŸºäºDAGæ¨¡æ¿çš„é¢„åŠ è½½ç­‰åŠŸèƒ½ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. å¤šçº§ç¼“å­˜æ¶æ„ (Redis + Memory + Computation)
2. æ™ºèƒ½TTLç®¡ç†
3. åŸºäºDAGæ¨¡æ¿çš„é¢„åŠ è½½æœºåˆ¶ï¼ˆv3.0æ–°å¢ï¼‰
4. ç¼“å­˜ä¸€è‡´æ€§éªŒè¯ï¼ˆv3.0æ–°å¢ï¼‰
5. ç¼“å­˜å¤±æ•ˆç­–ç•¥
6. æ€§èƒ½ç›‘æ§

ä½œè€…: BUILD_BODY Team
ç‰ˆæœ¬: v3.0.0
æ—¥æœŸ: 2025-12-12
"""

import asyncio
import logging
import time
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import pickle

logger = logging.getLogger(__name__)


class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«"""
    L1_MEMORY = "l1_memory"    # å†…å­˜ç¼“å­˜
    L2_REDIS = "l2_redis"      # Redisç¼“å­˜
    L3_COMPUTED = "l3_computed"  # è®¡ç®—ç¼“å­˜


class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥"""
    TIME_BASED = "time_based"        # åŸºäºæ—¶é—´
    ACCESS_BASED = "access_based"    # åŸºäºè®¿é—®
    SIZE_BASED = "size_based"        # åŸºäºå¤§å°
    INTELLIGENT = "intelligent"      # æ™ºèƒ½ç­–ç•¥


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    enable_memory_cache: bool = True
    enable_redis_cache: bool = True
    memory_cache_size: int = 1000
    memory_cache_ttl: int = 3600  # 1å°æ—¶
    redis_cache_ttl: int = 7200   # 2å°æ—¶
    enable_preloading: bool = True
    preload_threshold: float = 0.7  # é¢„åŠ è½½ç½®ä¿¡åº¦é˜ˆå€¼
    cleanup_interval: int = 300     # æ¸…ç†é—´éš”(ç§’)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    data: Any
    level: CacheLevel
    created_at: float
    last_accessed: float
    access_count: int = 0
    ttl: int = 3600
    size: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class CacheStatistics:
    """ç¼“å­˜ç»Ÿè®¡"""
    hits: int = 0
    misses: int = 0
    hit_rate: float = 0.0
    preloads: int = 0
    evictions: int = 0
    total_size: int = 0
    average_access_time: float = 0.0


class IntelligentCacheSystem:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, redis_client=None, config: CacheConfig = None):
        self.redis = redis_client
        self.config = config or CacheConfig()
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.access_patterns: Dict[str, List[float]] = defaultdict(list)
        self.cache_stats = CacheStatistics()
        self.tool_specific_configs = self._initialize_tool_configs()
        self.cleanup_task = None

        # å¯åŠ¨æ¸…ç†ä»»åŠ¡
        if self.config.enable_memory_cache:
            self._start_cleanup_task()

    def _initialize_tool_configs(self) -> Dict[str, Dict[str, Any]]:
        """åˆå§‹åŒ–å·¥å…·ç‰¹å®šé…ç½®"""
        return {
            # åŸºç¡€å·¥å…· - é•¿ç¼“å­˜
            "get_user_profile": {
                "ttl": 3600,      # 1å°æ—¶
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L2_REDIS  # ç”¨æˆ·æ¡£æ¡ˆé‡è¦ï¼Œæ”¾åœ¨Redis
            },
            "tdee_calculator": {
                "ttl": 1800,      # 30åˆ†é’Ÿ
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "chinese_food_analyzer": {
                "ttl": 7200,      # 2å°æ—¶
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "weight_calculator": {
                "ttl": 3600,      # 1å°æ—¶
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "rpe_recommender": {
                "ttl": 1800,      # 30åˆ†é’Ÿ
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },

            # å®‰å…¨å·¥å…· - çŸ­ç¼“å­˜
            "contraindications_checker": {
                "ttl": 300,       # 5åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "injury_risk_assessor": {
                "ttl": 300,       # 5åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "advanced_safety_monitor": {
                "ttl": 300,       # 5åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },

            # åŠ¨ä½œå·¥å…· - ä¸­ç­‰ç¼“å­˜
            "intelligent_exercise_selector": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": True,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "exercise_alternative_finder": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "safe_exercise_modifier": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "movement_pattern_balancer": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },

            # è®­ç»ƒè§„åˆ’å·¥å…· - æ— ç¼“å­˜
            "professional_program_designer": {
                "ttl": 0,         # ä¸ç¼“å­˜
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L3_COMPUTED
            },
            "periodized_program_designer": {
                "ttl": 0,         # ä¸ç¼“å­˜
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L3_COMPUTED
            },
            "training_split_designer": {
                "ttl": 0,         # ä¸ç¼“å­˜
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L3_COMPUTED
            },
            "muscle_group_volume_calculator": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "intelligent_weight_calculator": {
                "ttl": 300,       # 5åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },

            # è¥å…»å·¥å…· - ä¸­ç­‰ç¼“å­˜
            "nutrition_intake_analyzer": {
                "ttl": 1800,      # 30åˆ†é’Ÿ
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "exercise_nutrition_optimization": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "muscle_recovery_nutrition": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "nutrition_timing": {
                "ttl": 600,       # 10åˆ†é’Ÿ
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "meal_plan_designer": {
                "ttl": 0,         # ä¸ç¼“å­˜
                "preload": False,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L3_COMPUTED
            },

            # åˆ†æå·¥å…· - é•¿ç¼“å­˜
            "training_analytics_dashboard": {
                "ttl": 3600,      # 1å°æ—¶
                "preload": False,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },
            "evidence_based_recommender": {
                "ttl": 3600,      # 1å°æ—¶
                "preload": True,
                "strategy": CacheStrategy.ACCESS_BASED,
                "level": CacheLevel.L1_MEMORY
            },

            # è¾…åŠ©å·¥å…·
            "assess_strength_level": {
                "ttl": 7200,      # 2å°æ—¶
                "preload": True,
                "strategy": CacheStrategy.TIME_BASED,
                "level": CacheLevel.L1_MEMORY
            }
        }

    async def get(self, tool_name: str, params: Dict[str, Any], user_id: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(tool_name, params, user_id)
        start_time = time.time()

        try:
            # 1. å°è¯•L1å†…å­˜ç¼“å­˜
            if self.config.enable_memory_cache and tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[tool_name]
                if tool_config.get("level") == CacheLevel.L1_MEMORY:
                    result = await self._get_from_memory(cache_key)
                    if result is not None:
                        self._update_access_pattern(cache_key, start_time)
                        self.cache_stats.hits += 1
                        logger.debug(f"ğŸ’¾ L1ç¼“å­˜å‘½ä¸­: {tool_name}")
                        return result

            # 2. å°è¯•L2 Redisç¼“å­˜
            if self.config.enable_redis_cache and self.redis and tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[tool_name]
                if tool_config.get("level") == CacheLevel.L2_REDIS:
                    result = await self._get_from_redis(cache_key)
                    if result is not None:
                        # å›å¡«L1ç¼“å­˜
                        if self.config.enable_memory_cache:
                            await self._put_to_memory(cache_key, result, tool_config.get("ttl", 3600))
                        self._update_access_pattern(cache_key, start_time)
                        self.cache_stats.hits += 1
                        logger.debug(f"ğŸ’¾ L2ç¼“å­˜å‘½ä¸­: {tool_name}")
                        return result
            
            # 3. å¦‚æœRedisä¸å¯ç”¨ï¼Œå°è¯•ä»å†…å­˜ç¼“å­˜è·å–ï¼ˆé™çº§ç­–ç•¥ï¼‰
            if tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[tool_name]
                if tool_config.get("level") == CacheLevel.L2_REDIS and not self.redis:
                    # Redisä¸å¯ç”¨ï¼Œå°è¯•ä»å†…å­˜ç¼“å­˜è·å–
                    if self.config.enable_memory_cache:
                        result = await self._get_from_memory(cache_key)
                        if result is not None:
                            self._update_access_pattern(cache_key, start_time)
                            self.cache_stats.hits += 1
                            logger.debug(f"ğŸ’¾ L1ç¼“å­˜å‘½ä¸­(Redisé™çº§): {tool_name}")
                            return result

            # ç¼“å­˜æœªå‘½ä¸­
            self.cache_stats.misses += 1
            logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {tool_name}")
            return None

        except Exception as e:
            logger.error(f"ç¼“å­˜è·å–å¼‚å¸¸: {tool_name}, {e}")
            return None

    async def put(self, tool_name: str, params: Dict[str, Any], user_id: str, data: Any):
        """å­˜å‚¨ç¼“å­˜æ•°æ®"""
        if tool_name not in self.tool_specific_configs:
            return

        cache_key = self._generate_cache_key(tool_name, params, user_id)
        tool_config = self.tool_specific_configs[tool_name]
        ttl = tool_config.get("ttl", 3600)

        try:
            # å­˜å‚¨åˆ°L1å†…å­˜ç¼“å­˜
            if self.config.enable_memory_cache and tool_config.get("level") == CacheLevel.L1_MEMORY:
                await self._put_to_memory(cache_key, data, ttl)

            # å­˜å‚¨åˆ°L2 Redisç¼“å­˜
            if self.config.enable_redis_cache and self.redis and tool_config.get("level") == CacheLevel.L2_REDIS:
                await self._put_to_redis(cache_key, data, ttl)
            
            # å¦‚æœRedisä¸å¯ç”¨ï¼Œé™çº§åˆ°å†…å­˜ç¼“å­˜
            if tool_config.get("level") == CacheLevel.L2_REDIS and not self.redis:
                if self.config.enable_memory_cache:
                    await self._put_to_memory(cache_key, data, ttl)
                    logger.debug(f"âœ… ç¼“å­˜å­˜å‚¨æˆåŠŸ(Redisé™çº§åˆ°å†…å­˜): {tool_name}")
                    return

            logger.debug(f"âœ… ç¼“å­˜å­˜å‚¨æˆåŠŸ: {tool_name}")

        except Exception as e:
            logger.error(f"ç¼“å­˜å­˜å‚¨å¼‚å¸¸: {tool_name}, {e}")

    async def _get_from_memory(self, cache_key: str) -> Optional[Any]:
        """ä»å†…å­˜ç¼“å­˜è·å–"""
        if cache_key not in self.memory_cache:
            return None

        entry = self.memory_cache[cache_key]

        # æ£€æŸ¥TTL
        if time.time() - entry.created_at > entry.ttl:
            del self.memory_cache[cache_key]
            return None

        # æ›´æ–°è®¿é—®ä¿¡æ¯
        entry.last_accessed = time.time()
        entry.access_count += 1

        return entry.data

    async def _put_to_memory(self, cache_key: str, data: Any, ttl: int):
        """å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜"""
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.memory_cache) >= self.config.memory_cache_size:
            await self._evict_from_memory()

        entry = CacheEntry(
            key=cache_key,
            data=data,
            level=CacheLevel.L1_MEMORY,
            created_at=time.time(),
            last_accessed=time.time(),
            ttl=ttl,
            size=self._calculate_size(data)
        )

        self.memory_cache[cache_key] = entry
        self.cache_stats.total_size += entry.size

    async def _get_from_redis(self, cache_key: str) -> Optional[Any]:
        """ä»Redisç¼“å­˜è·å–"""
        try:
            cached_data = await self.redis.get(cache_key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            logger.warning(f"Redisè·å–å¤±è´¥: {e}")
        return None

    async def _put_to_redis(self, cache_key: str, data: Any, ttl: int):
        """å­˜å‚¨åˆ°Redisç¼“å­˜"""
        try:
            serialized_data = pickle.dumps(data)
            await self.redis.setex(cache_key, ttl, serialized_data)
        except Exception as e:
            logger.warning(f"Rediså­˜å‚¨å¤±è´¥: {e}")

    async def _evict_from_memory(self):
        """ä»å†…å­˜ç¼“å­˜æ·˜æ±°"""
        if not self.memory_cache:
            return

        # ä½¿ç”¨LRUç­–ç•¥æ·˜æ±°
        oldest_key = min(self.memory_cache.keys(), key=lambda k: self.memory_cache[k].last_accessed)
        evicted_entry = self.memory_cache.pop(oldest_key)
        self.cache_stats.total_size -= evicted_entry.size
        self.cache_stats.evictions += 1

        logger.debug(f"ğŸ—‘ï¸ LRUæ·˜æ±°: {oldest_key}")

    def _generate_cache_key(self, tool_name: str, params: Dict[str, Any], user_id: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æå–å…³é”®å‚æ•°
        key_params = self._extract_key_params(tool_name, params)
        key_data = {
            "tool": tool_name,
            "user_id": user_id,
            "params": key_params
        }

        key_str = json.dumps(key_data, sort_keys=True, default=str)
        return f"fitness_cache:{hashlib.md5(key_str.encode()).hexdigest()}"

    def _extract_key_params(self, tool_name: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®å‚æ•°"""
        key_params = {}

        if tool_name == "get_user_profile":
            key_params = {"user_id": params.get("user_id")}
        elif tool_name == "tdee_calculator":
            key_params = {
                "weight": params.get("weight"),
                "height": params.get("height"),
                "age": params.get("age"),
                "gender": params.get("gender"),
                "activity_level": params.get("activity_level")
            }
        elif tool_name == "intelligent_exercise_selector":
            key_params = {
                "muscle_group": params.get("muscle_group"),
                "training_goal": params.get("training_goal"),
                "difficulty_level": params.get("difficulty_level"),
                "available_equipment": params.get("available_equipment", [])
            }
        elif tool_name == "meal_plan_designer":
            key_params = {
                "user_id": params.get("user_id"),
                "dietary_preferences": params.get("dietary_preferences", []),
                "meals_per_day": params.get("meals_per_day")
            }
        else:
            # é»˜è®¤ï¼šä½¿ç”¨æ‰€æœ‰å‚æ•°
            key_params = params

        return key_params

    def _calculate_size(self, data: Any) -> int:
        """è®¡ç®—æ•°æ®å¤§å°"""
        try:
            return len(pickle.dumps(data))
        except:
            return 0

    def _update_access_pattern(self, cache_key: str, access_time: float):
        """æ›´æ–°è®¿é—®æ¨¡å¼"""
        self.access_patterns[cache_key].append(access_time)
        # ä¿ç•™æœ€è¿‘100æ¬¡è®¿é—®è®°å½•
        if len(self.access_patterns[cache_key]) > 100:
            self.access_patterns[cache_key] = self.access_patterns[cache_key][-100:]

    def _start_cleanup_task(self):
        """å¯åŠ¨æ¸…ç†ä»»åŠ¡"""
        async def cleanup_loop():
            while True:
                try:
                    await asyncio.sleep(self.config.cleanup_interval)
                    await self._cleanup_expired_entries()
                except Exception as e:
                    logger.error(f"ç¼“å­˜æ¸…ç†å¼‚å¸¸: {e}")

        # åªåœ¨æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯æ—¶å¯åŠ¨æ¸…ç†ä»»åŠ¡
        try:
            loop = asyncio.get_running_loop()
            self.cleanup_task = asyncio.create_task(cleanup_loop())
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç¨åæ‰‹åŠ¨å¯åŠ¨
            self.cleanup_task = None
            logger.debug("ç¼“å­˜æ¸…ç†ä»»åŠ¡å°†åœ¨äº‹ä»¶å¾ªç¯å¯ç”¨æ—¶å¯åŠ¨")

    async def _cleanup_expired_entries(self):
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        current_time = time.time()
        expired_keys = []

        for cache_key, entry in self.memory_cache.items():
            if current_time - entry.created_at > entry.ttl:
                expired_keys.append(cache_key)

        for key in expired_keys:
            entry = self.memory_cache.pop(key)
            self.cache_stats.total_size -= entry.size
            self.cache_stats.evictions += 1

        if expired_keys:
            logger.debug(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)}ä¸ªæ¡ç›®")

    async def preload_likely_data(
        self,
        intent_result: Dict[str, Any],
        user_profile: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é¢„åŠ è½½å¯èƒ½éœ€è¦çš„æ•°æ®ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        preloaded_data = {}
        confidence = intent_result.get("confidence", 0.0)

        if confidence < self.config.preload_threshold:
            return preloaded_data

        logger.info(f"ğŸš€ å¼€å§‹é¢„åŠ è½½æ•°æ®ï¼Œç½®ä¿¡åº¦: {confidence:.2f}")

        # åŸºäºæ„å›¾é¢„åŠ è½½
        required_tools = intent_result.get("required_tools", [])
        optional_tools = intent_result.get("optional_tools", [])

        # ä¼˜å…ˆé¢„åŠ è½½é«˜ç½®ä¿¡åº¦çš„å¿…éœ€å·¥å…·
        for tool_name in required_tools:
            if tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[tool_name]
                if tool_config.get("preload", False):
                    try:
                        # æ„å»ºé¢„åŠ è½½å‚æ•°
                        preload_params = self._build_preload_params(tool_name, user_profile)
                        preloaded_result = await self._execute_preload(tool_name, preload_params, user_profile)

                        if preloaded_result:
                            preloaded_data[tool_name] = preloaded_result
                            self.cache_stats.preloads += 1
                            logger.debug(f"âœ… é¢„åŠ è½½æˆåŠŸ: {tool_name}")

                    except Exception as e:
                        logger.warning(f"é¢„åŠ è½½å¤±è´¥: {tool_name}, {e}")

        return preloaded_data

    async def preload_from_dag_template(
        self,
        template,
        user_profile: Dict[str, Any],
        user_id: str,
        execute_func: callable = None
    ) -> Dict[str, Any]:
        """
        åŸºäºDAGæ¨¡æ¿çš„æ™ºèƒ½é¢„åŠ è½½ï¼ˆv3.0æ–°å¢ï¼‰
        
        æ ¹æ®DAGæ¨¡æ¿çš„å·¥å…·é“¾ï¼Œæ™ºèƒ½é¢„åŠ è½½å¯èƒ½éœ€è¦çš„æ•°æ®ã€‚
        
        Args:
            template: DAGæ¨¡æ¿å¯¹è±¡
            user_profile: ç”¨æˆ·æ¡£æ¡ˆ
            user_id: ç”¨æˆ·ID
            execute_func: å·¥å…·æ‰§è¡Œå‡½æ•°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, Any]: é¢„åŠ è½½çš„æ•°æ®
        """
        preloaded_data = {}
        
        if not self.config.enable_preloading:
            return preloaded_data
        
        logger.info(f"ğŸš€ åŸºäºDAGæ¨¡æ¿é¢„åŠ è½½: {template.name}")
        
        # 1. è¯†åˆ«å¯é¢„åŠ è½½çš„å·¥å…·
        preloadable_tools = self._identify_preloadable_tools(template)
        
        if not preloadable_tools:
            logger.debug("æ²¡æœ‰å¯é¢„åŠ è½½çš„å·¥å…·")
            return preloaded_data
        
        logger.info(f"ğŸ“¦ è¯†åˆ«åˆ° {len(preloadable_tools)} ä¸ªå¯é¢„åŠ è½½å·¥å…·")
        
        # 2. æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆåŸºç¡€å·¥å…·ä¼˜å…ˆï¼‰
        sorted_tools = self._sort_tools_by_priority(preloadable_tools, template)
        
        # 3. å¹¶è¡Œé¢„åŠ è½½ï¼ˆæ— ä¾èµ–çš„å·¥å…·ï¼‰
        preload_tasks = []
        for tool_name in sorted_tools:
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
            tool_params = self._build_preload_params(tool_name, user_profile)
            cache_key = self._generate_cache_key(tool_name, tool_params, user_id)
            
            # å¦‚æœå·²ç¼“å­˜ï¼Œè·³è¿‡
            cached_result = await self.get(tool_name, tool_params, user_id)
            if cached_result is not None:
                preloaded_data[tool_name] = cached_result
                logger.debug(f"âœ… ä½¿ç”¨å·²ç¼“å­˜æ•°æ®: {tool_name}")
                continue
            
            # å¦‚æœæä¾›äº†æ‰§è¡Œå‡½æ•°ï¼Œåˆ›å»ºé¢„åŠ è½½ä»»åŠ¡
            if execute_func:
                task = self._create_preload_task(
                    tool_name,
                    tool_params,
                    user_id,
                    execute_func
                )
                preload_tasks.append(task)
        
        # 4. æ‰§è¡Œé¢„åŠ è½½ä»»åŠ¡
        if preload_tasks:
            logger.info(f"âš¡ å¹¶è¡Œæ‰§è¡Œ {len(preload_tasks)} ä¸ªé¢„åŠ è½½ä»»åŠ¡")
            results = await asyncio.gather(*preload_tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.warning(f"é¢„åŠ è½½ä»»åŠ¡å¤±è´¥: {result}")
                elif result:
                    tool_name, data = result
                    preloaded_data[tool_name] = data
                    self.cache_stats.preloads += 1
                    logger.debug(f"âœ… é¢„åŠ è½½æˆåŠŸ: {tool_name}")
        
        logger.info(f"âœ… DAGæ¨¡æ¿é¢„åŠ è½½å®Œæˆ: {len(preloaded_data)} ä¸ªå·¥å…·")
        return preloaded_data
    
    def _identify_preloadable_tools(self, template) -> List[str]:
        """è¯†åˆ«å¯é¢„åŠ è½½çš„å·¥å…·"""
        preloadable = []
        
        # æ£€æŸ¥å¿…éœ€å·¥å…·
        for tool_name in template.required_tools:
            # è½¬æ¢å·¥å…·åæ ¼å¼
            registry_tool_name = tool_name.replace('-', '_')
            
            if registry_tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[registry_tool_name]
                if tool_config.get("preload", False) and tool_config.get("ttl", 0) > 0:
                    preloadable.append(registry_tool_name)
        
        # æ£€æŸ¥å¯é€‰å·¥å…·ï¼ˆé€‰æ‹©æ€§é¢„åŠ è½½ï¼‰
        for tool_name in template.optional_tools[:2]:  # åªé¢„åŠ è½½å‰2ä¸ªå¯é€‰å·¥å…·
            registry_tool_name = tool_name.replace('-', '_')
            
            if registry_tool_name in self.tool_specific_configs:
                tool_config = self.tool_specific_configs[registry_tool_name]
                if tool_config.get("preload", False) and tool_config.get("ttl", 0) > 0:
                    preloadable.append(registry_tool_name)
        
        return preloadable
    
    def _sort_tools_by_priority(self, tools: List[str], template) -> List[str]:
        """æŒ‰ä¼˜å…ˆçº§æ’åºå·¥å…·"""
        # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
        priority_order = {
            "get_user_profile": 1,
            "tdee_calculator": 2,
            "assess_strength_level": 3,
            "rpe_recommender": 4,
            "weight_calculator": 5,
            "intelligent_exercise_selector": 6,
            "nutrition_intake_analyzer": 7,
            "evidence_based_recommender": 8
        }
        
        return sorted(tools, key=lambda t: priority_order.get(t, 99))
    
    async def _create_preload_task(
        self,
        tool_name: str,
        params: Dict[str, Any],
        user_id: str,
        execute_func: callable
    ):
        """åˆ›å»ºé¢„åŠ è½½ä»»åŠ¡"""
        try:
            # æ‰§è¡Œå·¥å…·
            result = await execute_func(tool_name, params)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            await self.put(tool_name, params, user_id, result)
            
            return (tool_name, result)
        except Exception as e:
            logger.warning(f"é¢„åŠ è½½ä»»åŠ¡å¤±è´¥: {tool_name}, {e}")
            return None
    
    async def validate_cache_consistency(
        self,
        tool_name: str,
        params: Dict[str, Any],
        user_id: str,
        fresh_result: Any
    ) -> Dict[str, Any]:
        """
        éªŒè¯ç¼“å­˜ä¸€è‡´æ€§ï¼ˆv3.0æ–°å¢ï¼‰
        
        æ¯”è¾ƒç¼“å­˜ç»“æœå’Œæ–°é²œç»“æœï¼Œæ£€æµ‹æ•°æ®æ¼‚ç§»ã€‚
        
        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°
            user_id: ç”¨æˆ·ID
            fresh_result: æ–°é²œçš„æ‰§è¡Œç»“æœ
            
        Returns:
            Dict[str, Any]: éªŒè¯ç»“æœ
        """
        cache_key = self._generate_cache_key(tool_name, params, user_id)
        cached_result = await self.get(tool_name, params, user_id)
        
        validation_result = {
            "tool_name": tool_name,
            "has_cached": cached_result is not None,
            "is_consistent": False,
            "drift_detected": False,
            "differences": [],
            "recommendation": "use_fresh"
        }
        
        if cached_result is None:
            validation_result["recommendation"] = "use_fresh"
            return validation_result
        
        # æ¯”è¾ƒç»“æœ
        try:
            is_consistent, differences = self._compare_results(
                cached_result,
                fresh_result,
                tool_name
            )
            
            validation_result["is_consistent"] = is_consistent
            validation_result["differences"] = differences
            
            if is_consistent:
                validation_result["recommendation"] = "use_cached"
            else:
                validation_result["drift_detected"] = True
                validation_result["recommendation"] = "use_fresh_and_update_cache"
                
                # æ›´æ–°ç¼“å­˜
                await self.put(tool_name, params, user_id, fresh_result)
                logger.warning(
                    f"âš ï¸ æ£€æµ‹åˆ°ç¼“å­˜æ¼‚ç§»: {tool_name}, "
                    f"å·®å¼‚æ•°: {len(differences)}"
                )
        
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {tool_name}, {e}")
            validation_result["recommendation"] = "use_fresh"
        
        return validation_result
    
    def _compare_results(
        self,
        cached: Any,
        fresh: Any,
        tool_name: str
    ) -> Tuple[bool, List[str]]:
        """æ¯”è¾ƒä¸¤ä¸ªç»“æœ"""
        differences = []
        
        # å¦‚æœç±»å‹ä¸åŒï¼Œç›´æ¥åˆ¤å®šä¸ä¸€è‡´
        if type(cached) != type(fresh):
            differences.append(f"ç±»å‹ä¸åŒ: {type(cached)} vs {type(fresh)}")
            return False, differences
        
        # å­—å…¸ç±»å‹æ¯”è¾ƒ
        if isinstance(cached, dict) and isinstance(fresh, dict):
            return self._compare_dicts(cached, fresh, tool_name)
        
        # åˆ—è¡¨ç±»å‹æ¯”è¾ƒ
        elif isinstance(cached, list) and isinstance(fresh, list):
            return self._compare_lists(cached, fresh, tool_name)
        
        # åŸºæœ¬ç±»å‹æ¯”è¾ƒ
        else:
            if cached != fresh:
                differences.append(f"å€¼ä¸åŒ: {cached} vs {fresh}")
                return False, differences
            return True, []
    
    def _compare_dicts(
        self,
        cached: Dict,
        fresh: Dict,
        tool_name: str
    ) -> Tuple[bool, List[str]]:
        """æ¯”è¾ƒå­—å…¸"""
        differences = []
        
        # æ£€æŸ¥é”®é›†åˆ
        cached_keys = set(cached.keys())
        fresh_keys = set(fresh.keys())
        
        if cached_keys != fresh_keys:
            missing_in_fresh = cached_keys - fresh_keys
            missing_in_cached = fresh_keys - cached_keys
            
            if missing_in_fresh:
                differences.append(f"æ–°ç»“æœç¼ºå°‘é”®: {missing_in_fresh}")
            if missing_in_cached:
                differences.append(f"ç¼“å­˜ç¼ºå°‘é”®: {missing_in_cached}")
        
        # æ¯”è¾ƒå…±åŒé”®çš„å€¼
        common_keys = cached_keys & fresh_keys
        for key in common_keys:
            # è·³è¿‡æ—¶é—´æˆ³å­—æ®µ
            if key in ['created_at', 'updated_at', 'timestamp', 'last_modified']:
                continue
            
            cached_val = cached[key]
            fresh_val = fresh[key]
            
            # æ•°å€¼ç±»å‹ï¼šå…è®¸å°è¯¯å·®
            if isinstance(cached_val, (int, float)) and isinstance(fresh_val, (int, float)):
                if abs(cached_val - fresh_val) > 0.01:  # 1%è¯¯å·®
                    differences.append(f"é”® '{key}' å€¼å·®å¼‚: {cached_val} vs {fresh_val}")
            
            # å…¶ä»–ç±»å‹ï¼šç²¾ç¡®æ¯”è¾ƒ
            elif cached_val != fresh_val:
                differences.append(f"é”® '{key}' å€¼ä¸åŒ")
        
        is_consistent = len(differences) == 0
        return is_consistent, differences
    
    def _compare_lists(
        self,
        cached: List,
        fresh: List,
        tool_name: str
    ) -> Tuple[bool, List[str]]:
        """æ¯”è¾ƒåˆ—è¡¨"""
        differences = []
        
        # é•¿åº¦æ¯”è¾ƒ
        if len(cached) != len(fresh):
            differences.append(f"åˆ—è¡¨é•¿åº¦ä¸åŒ: {len(cached)} vs {len(fresh)}")
            return False, differences
        
        # å…ƒç´ æ¯”è¾ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
        for i, (c_item, f_item) in enumerate(zip(cached, fresh)):
            if c_item != f_item:
                differences.append(f"ç´¢å¼• {i} å…ƒç´ ä¸åŒ")
        
        is_consistent = len(differences) == 0
        return is_consistent, differences

    def _build_preload_params(self, tool_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºé¢„åŠ è½½å‚æ•°"""
        base_params = {"user_profile": user_profile, "user_id": user_profile.get("user_id")}

        if tool_name == "tdee_calculator":
            base_params.update({
                "weight": user_profile.get("weight"),
                "height": user_profile.get("height"),
                "age": user_profile.get("age"),
                "gender": user_profile.get("gender"),
                "activity_level": user_profile.get("activity_level", "moderate")
            })
        elif tool_name == "intelligent_exercise_selector":
            base_params.update({
                "muscle_group": user_profile.get("target_muscle_groups", ["èƒ¸éƒ¨"])[0],
                "training_goal": user_profile.get("fitness_goals", ["å¢è‚Œ"])[0],
                "available_equipment": user_profile.get("available_equipment", ["å“‘é“ƒ"]),
                "difficulty_level": user_profile.get("fitness_level", "beginner")
            })
        elif tool_name == "evidence_based_recommender":
            base_params.update({
                "query": f"é’ˆå¯¹{user_profile.get('fitness_goals', ['å¥èº«'])[0]}çš„å»ºè®®",
                "preference": "balanced"
            })

        return base_params

    async def _execute_preload(self, tool_name: str, params: Dict[str, Any], user_id: str) -> Optional[Any]:
        """æ‰§è¡Œé¢„åŠ è½½"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å·¥å…·æ‰§è¡Œé€»è¾‘
        # æš‚æ—¶è¿”å›Noneï¼Œè¡¨ç¤ºéœ€è¦å®é™…å®ç°
        return None

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_stats.hits + self.cache_stats.misses
        if total_requests > 0:
            self.cache_stats.hit_rate = (self.cache_stats.hits / total_requests) * 100

        return {
            "stats": {
                "hits": self.cache_stats.hits,
                "misses": self.cache_stats.misses,
                "hit_rate": self.cache_stats.hit_rate,
                "preloads": self.cache_stats.preloads,
                "evictions": self.cache_stats.evictions,
                "total_size": self.cache_stats.total_size,
                "memory_cache_size": len(self.memory_cache)
            },
            "tool_configs": {
                tool: {
                    "ttl": config.get("ttl"),
                    "preload": config.get("preload"),
                    "level": config.get("level").value if config.get("level") else None
                }
                for tool, config in self.tool_specific_configs.items()
            },
            "access_patterns": {
                key: {
                    "access_count": len(pattern),
                    "last_access": pattern[-1] if pattern else None
                }
                for key, pattern in self.access_patterns.items()
            }
        }

    async def clear_cache(self, tool_name: Optional[str] = None):
        """æ¸…ç†ç¼“å­˜"""
        if tool_name:
            # æ¸…ç†ç‰¹å®šå·¥å…·çš„ç¼“å­˜
            keys_to_remove = [
                key for key in self.memory_cache.keys()
                if key.endswith(f":{tool_name}")
            ]
            for key in keys_to_remove:
                entry = self.memory_cache.pop(key)
                self.cache_stats.total_size -= entry.size
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            self.memory_cache.clear()
            if self.redis:
                await self.redis.flushdb()

        logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: {tool_name or 'å…¨éƒ¨'}")

    async def shutdown(self):
        """å…³é—­ç¼“å­˜ç³»ç»Ÿ"""
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass

        logger.info("ğŸ”„ ç¼“å­˜ç³»ç»Ÿå·²å…³é—­")


class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ï¼ˆv3.0å¢å¼ºï¼‰"""

    def __init__(self, cache_system: IntelligentCacheSystem):
        self.cache_system = cache_system
        self.user_patterns = defaultdict(dict)
        self.preload_history = defaultdict(list)  # v3.0æ–°å¢ï¼šé¢„åŠ è½½å†å²

    async def get_tool_result(
        self,
        tool_name: str,
        params: Dict[str, Any],
        user_id: str,
        execute_func: callable
    ) -> Any:
        """æ™ºèƒ½è·å–å·¥å…·ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self.cache_system.get(tool_name, params, user_id)
        if cached_result is not None:
            return cached_result

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå·¥å…·
        logger.info(f"âš¡ æ‰§è¡Œå·¥å…·: {tool_name}")
        result = await execute_func(params)

        # å­˜å‚¨åˆ°ç¼“å­˜
        await self.cache_system.put(tool_name, params, user_id, result)

        return result

    async def preload_user_context(
        self,
        intent_result: Dict[str, Any],
        user_profile: Dict[str, Any],
        user_id: str
    ) -> Dict[str, Any]:
        """é¢„åŠ è½½ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        return await self.cache_system.preload_likely_data(intent_result, user_profile)

    async def preload_from_template(
        self,
        template,
        user_profile: Dict[str, Any],
        user_id: str,
        execute_func: callable = None
    ) -> Dict[str, Any]:
        """
        åŸºäºDAGæ¨¡æ¿é¢„åŠ è½½ï¼ˆv3.0æ–°å¢ï¼‰
        
        Args:
            template: DAGæ¨¡æ¿å¯¹è±¡
            user_profile: ç”¨æˆ·æ¡£æ¡ˆ
            user_id: ç”¨æˆ·ID
            execute_func: å·¥å…·æ‰§è¡Œå‡½æ•°
            
        Returns:
            Dict[str, Any]: é¢„åŠ è½½çš„æ•°æ®
        """
        start_time = time.time()
        
        # è°ƒç”¨ç¼“å­˜ç³»ç»Ÿçš„é¢„åŠ è½½æ–¹æ³•
        preloaded_data = await self.cache_system.preload_from_dag_template(
            template=template,
            user_profile=user_profile,
            user_id=user_id,
            execute_func=execute_func
        )
        
        # è®°å½•é¢„åŠ è½½å†å²
        preload_record = {
            "timestamp": start_time,
            "template_id": template.template_id,
            "template_name": template.name,
            "tools_preloaded": list(preloaded_data.keys()),
            "preload_count": len(preloaded_data),
            "duration": time.time() - start_time
        }
        self.preload_history[user_id].append(preload_record)
        
        # ä¿ç•™æœ€è¿‘10æ¡è®°å½•
        if len(self.preload_history[user_id]) > 10:
            self.preload_history[user_id] = self.preload_history[user_id][-10:]
        
        logger.info(
            f"âœ… æ¨¡æ¿é¢„åŠ è½½å®Œæˆ: {template.name}, "
            f"å·¥å…·æ•°: {len(preloaded_data)}, "
            f"è€—æ—¶: {preload_record['duration']:.2f}s"
        )
        
        return preloaded_data

    async def validate_and_refresh_cache(
        self,
        tool_name: str,
        params: Dict[str, Any],
        user_id: str,
        fresh_result: Any
    ) -> Dict[str, Any]:
        """
        éªŒè¯å¹¶åˆ·æ–°ç¼“å­˜ï¼ˆv3.0æ–°å¢ï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°
            params: å·¥å…·å‚æ•°
            user_id: ç”¨æˆ·ID
            fresh_result: æ–°é²œçš„æ‰§è¡Œç»“æœ
            
        Returns:
            Dict[str, Any]: éªŒè¯ç»“æœ
        """
        validation_result = await self.cache_system.validate_cache_consistency(
            tool_name=tool_name,
            params=params,
            user_id=user_id,
            fresh_result=fresh_result
        )
        
        # å¦‚æœæ£€æµ‹åˆ°æ¼‚ç§»ï¼Œè®°å½•æ—¥å¿—
        if validation_result.get("drift_detected"):
            logger.warning(
                f"âš ï¸ ç¼“å­˜æ¼‚ç§»: {tool_name}, "
                f"å·®å¼‚: {len(validation_result.get('differences', []))}"
            )
        
        return validation_result

    def update_user_pattern(self, user_id: str, tool_name: str, usage_count: int = 1):
        """æ›´æ–°ç”¨æˆ·ä½¿ç”¨æ¨¡å¼"""
        if user_id not in self.user_patterns:
            self.user_patterns[user_id] = {}

        if tool_name not in self.user_patterns[user_id]:
            self.user_patterns[user_id][tool_name] = 0

        self.user_patterns[user_id][tool_name] += usage_count

    def get_user_pattern(self, user_id: str) -> Dict[str, int]:
        """è·å–ç”¨æˆ·ä½¿ç”¨æ¨¡å¼"""
        return self.user_patterns.get(user_id, {})

    def get_preload_history(self, user_id: str) -> List[Dict[str, Any]]:
        """è·å–é¢„åŠ è½½å†å²ï¼ˆv3.0æ–°å¢ï¼‰"""
        return self.preload_history.get(user_id, [])

    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ï¼ˆv3.0å¢å¼ºï¼‰"""
        base_stats = self.cache_system.get_statistics()
        
        # æ·»åŠ é¢„åŠ è½½ç»Ÿè®¡
        total_preloads = sum(
            len(history) for history in self.preload_history.values()
        )
        
        base_stats["preload_statistics"] = {
            "total_preload_sessions": total_preloads,
            "users_with_preload": len(self.preload_history),
            "average_tools_per_preload": (
                sum(
                    record["preload_count"]
                    for history in self.preload_history.values()
                    for record in history
                ) / total_preloads if total_preloads > 0 else 0
            )
        }
        
        return base_stats
