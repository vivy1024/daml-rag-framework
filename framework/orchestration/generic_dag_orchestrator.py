# -*- coding: utf-8 -*-
"""
é€šç”¨DAGç¼–æ’å™¨ - æ¡†æ¶å±‚æ ¸å¿ƒç»„ä»¶

æä¾›é¢†åŸŸæ— å…³çš„DAGç¼–æ’é€»è¾‘ï¼Œæ”¯æŒï¼š
1. æ‹“æ‰‘æ’åºï¼ˆKahnç®—æ³•ï¼‰
2. å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–
3. ä¾èµ–è§£æ
4. é”™è¯¯å¤„ç†å’Œé‡è¯•
5. ç¼“å­˜ç®¡ç†

ç‰¹ç‚¹ï¼š
- é¢†åŸŸæ— å…³ï¼šä¸åŒ…å«ä»»ä½•ä¸šåŠ¡é€»è¾‘
- å¯é…ç½®ï¼šé€šè¿‡ToolRegistryæ³¨å…¥å·¥å…·
- å¯æ‰©å±•ï¼šæ”¯æŒè‡ªå®šä¹‰æ‰§è¡Œç­–ç•¥

ä½œè€…: BUILD_BODY Team
ç‰ˆæœ¬: v1.0.0
æ—¥æœŸ: 2025-12-14
"""

import asyncio
import logging
import time
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque

from .tool_registry import ToolRegistry, ToolMetadata, TaskPriority

logger = logging.getLogger(__name__)


class TaskStatus(Enum):
    """ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"           # ç­‰å¾…æ‰§è¡Œ
    RUNNING = "running"           # æ‰§è¡Œä¸­
    COMPLETED = "completed"       # å·²å®Œæˆ
    FAILED = "failed"            # æ‰§è¡Œå¤±è´¥
    SKIPPED = "skipped"          # è·³è¿‡ï¼ˆä¾èµ–å¤±è´¥ï¼‰
    CANCELLED = "cancelled"      # å·²å–æ¶ˆ


@dataclass
class DAGTask:
    """DAGä»»åŠ¡å®šä¹‰"""
    tool_name: str
    tool_metadata: ToolMetadata
    params: Dict[str, Any]
    dependencies: List[str] = field(default_factory=list)
    status: TaskStatus = TaskStatus.PENDING
    priority: TaskPriority = TaskPriority.NORMAL
    retry_count: int = 0
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    execution_order: Optional[int] = None
    level: Optional[int] = None  # DAGå±‚çº§


@dataclass
class ExecutionLevel:
    """æ‰§è¡Œå±‚çº§"""
    level: int
    tasks: List[DAGTask]
    parallel_groups: List[List[DAGTask]] = field(default_factory=list)
    estimated_duration: float = 0.0
    can_parallel: bool = True


@dataclass
class DAGTemplate:
    """DAGæ¨¡æ¿å®šä¹‰"""
    template_id: str
    name: str
    description: str
    required_tools: List[str]
    optional_tools: List[str] = field(default_factory=list)
    tool_dependencies: Dict[str, List[str]] = field(default_factory=dict)
    parallel_groups: List[List[str]] = field(default_factory=list)
    complexity_level: int = 1  # 1-3
    estimated_duration_seconds: float = 0.0


@dataclass
class DAGExecutionResult:
    """DAGæ‰§è¡Œç»“æœ"""
    execution_id: str
    template_id: str
    success: bool
    total_time: float
    levels_executed: int
    tasks_completed: int
    tasks_failed: int
    results: Dict[str, Any] = field(default_factory=dict)
    errors: Dict[str, str] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)


class GenericDAGOrchestrator:
    """
    é€šç”¨DAGç¼–æ’å™¨ï¼ˆæ¡†æ¶å±‚ï¼‰
    
    ç‰¹ç‚¹ï¼š
    - é¢†åŸŸæ— å…³ï¼šä¸åŒ…å«ä»»ä½•ä¸šåŠ¡é€»è¾‘
    - å¯é…ç½®ï¼šé€šè¿‡ToolRegistryæ³¨å…¥å·¥å…·
    - å¯æ‰©å±•ï¼šæ”¯æŒè‡ªå®šä¹‰æ‰§è¡Œç­–ç•¥
    """
    
    def __init__(
        self,
        tool_registry: ToolRegistry,
        tool_executor: Optional[Callable] = None,
        cache_manager: Optional[Any] = None,
        resource_pools: Optional[Dict[str, asyncio.Semaphore]] = None
    ):
        """
        åˆå§‹åŒ–é€šç”¨DAGç¼–æ’å™¨
        
        Args:
            tool_registry: å·¥å…·æ³¨å†Œè¡¨ï¼ˆç”±åº”ç”¨å±‚æ³¨å…¥ï¼‰
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼ˆå¯é€‰ï¼Œç”¨äºå®é™…è°ƒç”¨å·¥å…·ï¼‰
            cache_manager: ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            resource_pools: èµ„æºæ± ï¼ˆå¯é€‰ï¼Œç”¨äºå¹¶å‘æ§åˆ¶ï¼‰
        """
        self.tool_registry = tool_registry
        self.tool_executor = tool_executor
        self.cache_manager = cache_manager
        self.resource_pools = resource_pools or {}
        
        self.execution_history = []
        self.performance_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "average_execution_time": 0.0,
            "cache_hit_rate": 0.0
        }
        
        logger.info(f"âœ… é€šç”¨DAGç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆï¼Œå·¥å…·æ³¨å†Œè¡¨åŒ…å« {len(tool_registry)} ä¸ªå·¥å…·")
    
    async def execute_template(
        self,
        template: DAGTemplate,
        context: Dict[str, Any],
        cached_results: Optional[Dict[str, Any]] = None
    ) -> DAGExecutionResult:
        """
        æ‰§è¡ŒDAGæ¨¡æ¿ï¼ˆé€šç”¨é€»è¾‘ï¼‰
        
        Args:
            template: DAGæ¨¡æ¿
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            cached_results: ç¼“å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
        
        Returns:
            DAGExecutionResult: æ‰§è¡Œç»“æœ
        """
        execution_id = self._generate_execution_id()
        start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹DAGæ‰§è¡Œ: {execution_id}")
        logger.info(f"ğŸ“‹ æ¨¡æ¿: {template.name} (ID: {template.template_id})")
        
        self.performance_stats["total_executions"] += 1
        
        try:
            # æ­¥éª¤1: ä»æ¨¡æ¿æ„å»ºä»»åŠ¡å›¾
            tasks = self._build_tasks_from_template(template, context)
            logger.info(f"ğŸ“¦ æ„å»ºäº† {len(tasks)} ä¸ªä»»åŠ¡")
            
            # æ­¥éª¤2: æ‹“æ‰‘æ’åº
            execution_levels = self._topological_sort(tasks, template.tool_dependencies)
            logger.info(f"ğŸ“Š æ‹“æ‰‘æ’åºå®Œæˆï¼Œå…± {len(execution_levels)} ä¸ªå±‚çº§")
            
            # æ­¥éª¤3: å¹¶è¡Œä¼˜åŒ–
            optimized_levels = self._optimize_parallel_execution(execution_levels, cached_results)
            logger.info(f"âš¡ å¹¶è¡Œä¼˜åŒ–å®Œæˆ")
            
            # æ­¥éª¤4: æ‰§è¡ŒDAG
            result = await self._execute_levels(
                execution_id,
                optimized_levels,
                context,
                cached_results
            )
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.performance_stats["successful_executions"] += 1
            execution_time = time.time() - start_time
            self._update_average_execution_time(execution_time)
            
            # è®°å½•æ‰§è¡Œå†å²
            self.execution_history.append({
                "execution_id": execution_id,
                "timestamp": start_time,
                "template_id": template.template_id,
                "template_name": template.name,
                "execution_time": execution_time,
                "success": result.success
            })
            
            logger.info(f"âœ… DAGæ‰§è¡ŒæˆåŠŸ: {execution_id}, è€—æ—¶: {execution_time:.2f}s")
            
            return result
            
        except Exception as e:
            self.performance_stats["failed_executions"] += 1
            logger.error(f"âŒ DAGæ‰§è¡Œå¤±è´¥: {execution_id}, é”™è¯¯: {e}", exc_info=True)
            
            return DAGExecutionResult(
                execution_id=execution_id,
                template_id=template.template_id,
                success=False,
                total_time=time.time() - start_time,
                levels_executed=0,
                tasks_completed=0,
                tasks_failed=1,
                errors={"execution": str(e)}
            )
    
    def _build_tasks_from_template(
        self,
        template: DAGTemplate,
        context: Dict[str, Any]
    ) -> List[DAGTask]:
        """
        ä»æ¨¡æ¿æ„å»ºä»»åŠ¡åˆ—è¡¨ï¼ˆä½¿ç”¨æ³¨å…¥çš„ToolRegistryï¼‰
        
        Args:
            template: DAGæ¨¡æ¿
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Returns:
            List[DAGTask]: ä»»åŠ¡åˆ—è¡¨
        """
        tasks = []
        
        # å¤„ç†å¿…éœ€å·¥å…·
        for tool_name in template.required_tools:
            # è½¬æ¢å·¥å…·åæ ¼å¼ï¼ˆkebab-case -> snake_caseï¼‰
            registry_tool_name = tool_name.replace('-', '_')
            
            # ä»æ³¨å†Œè¡¨è·å–å·¥å…·å…ƒæ•°æ®
            metadata = self.tool_registry.get_metadata(registry_tool_name)
            if not metadata:
                logger.warning(f"âš ï¸ å·¥å…·æœªæ³¨å†Œ: {tool_name}ï¼Œè·³è¿‡")
                continue
            
            # æ„å»ºä»»åŠ¡å‚æ•°
            params = self._build_task_params(registry_tool_name, context)
            
            # åˆ›å»ºä»»åŠ¡
            task = DAGTask(
                tool_name=registry_tool_name,
                tool_metadata=metadata,
                params=params,
                dependencies=[dep.replace('-', '_') for dep in template.tool_dependencies.get(tool_name, [])],
                priority=metadata.priority
            )
            tasks.append(task)
        
        # å¤„ç†å¯é€‰å·¥å…·
        for tool_name in template.optional_tools:
            registry_tool_name = tool_name.replace('-', '_')
            
            metadata = self.tool_registry.get_metadata(registry_tool_name)
            if not metadata:
                logger.warning(f"âš ï¸ å¯é€‰å·¥å…·æœªæ³¨å†Œ: {tool_name}ï¼Œè·³è¿‡")
                continue
            
            params = self._build_task_params(registry_tool_name, context)
            
            task = DAGTask(
                tool_name=registry_tool_name,
                tool_metadata=metadata,
                params=params,
                dependencies=[dep.replace('-', '_') for dep in template.tool_dependencies.get(tool_name, [])],
                priority=TaskPriority.LOW  # å¯é€‰å·¥å…·ä¼˜å…ˆçº§è¾ƒä½
            )
            tasks.append(task)
        
        # åˆ†é…æ‰§è¡Œé¡ºåº
        for i, task in enumerate(tasks):
            task.execution_order = i
        
        return tasks
    
    def _build_task_params(
        self,
        tool_name: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ„å»ºä»»åŠ¡å‚æ•°ï¼ˆé€šç”¨é€»è¾‘ï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Returns:
            Dict: ä»»åŠ¡å‚æ•°
        """
        # åŸºç¡€å‚æ•°
        params = {
            "tool_name": tool_name,
            "context": context
        }
        
        # ä»ä¸Šä¸‹æ–‡ä¸­æå–å¸¸ç”¨å‚æ•°
        if "user_profile" in context:
            params["user_profile"] = context["user_profile"]
        
        if "user_id" in context:
            params["user_id"] = context["user_id"]
        
        if "session_context" in context:
            params["session_context"] = context["session_context"]
        
        return params
    
    def _topological_sort(
        self,
        tasks: List[DAGTask],
        dependencies: Dict[str, List[str]]
    ) -> List[ExecutionLevel]:
        """
        æ‹“æ‰‘æ’åº - Kahnç®—æ³•
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            dependencies: ä¾èµ–å…³ç³»
        
        Returns:
            List[ExecutionLevel]: æ‰§è¡Œå±‚çº§åˆ—è¡¨
        """
        # è®¡ç®—å…¥åº¦
        in_degree = {task.tool_name: 0 for task in tasks}
        task_map = {task.tool_name: task for task in tasks}
        
        # è½¬æ¢ä¾èµ–å…³ç³»ä¸­çš„å·¥å…·åæ ¼å¼
        normalized_dependencies = {}
        for tool_name, deps in dependencies.items():
            normalized_tool_name = tool_name.replace('-', '_')
            normalized_deps = [dep.replace('-', '_') for dep in deps]
            normalized_dependencies[normalized_tool_name] = normalized_deps
        
        # è®¡ç®—å…¥åº¦
        for task in tasks:
            for dep in normalized_dependencies.get(task.tool_name, []):
                if dep in in_degree:
                    in_degree[task.tool_name] += 1
        
        # åˆå§‹åŒ–å±‚çº§
        levels = []
        remaining_tasks = set(task_map.keys())
        
        level_num = 0
        while remaining_tasks:
            # æ‰¾åˆ°å½“å‰å…¥åº¦ä¸º0çš„ä»»åŠ¡
            current_level_tasks = []
            for task_name in list(remaining_tasks):
                if in_degree[task_name] == 0:
                    current_level_tasks.append(task_map[task_name])
            
            if not current_level_tasks:
                # å­˜åœ¨å¾ªç¯ä¾èµ–ï¼Œé€‰æ‹©ä¸€ä¸ªä»»åŠ¡ç»§ç»­
                task_name = list(remaining_tasks)[0]
                current_level_tasks = [task_map[task_name]]
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œé€‰æ‹©ä»»åŠ¡ç»§ç»­: {task_name}")
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            current_level_tasks.sort(key=lambda t: t.priority.value)
            
            # åˆ›å»ºæ‰§è¡Œå±‚çº§
            level = ExecutionLevel(
                level=level_num,
                tasks=current_level_tasks,
                estimated_duration=max(task.tool_metadata.execution_time for task in current_level_tasks),
                can_parallel=all(task.tool_metadata.parallel_safe for task in current_level_tasks)
            )
            
            levels.append(level)
            
            # æ›´æ–°å…¥åº¦
            for task in current_level_tasks:
                remaining_tasks.remove(task.tool_name)
                task.level = level_num
                
                # å‡å°‘ä¾èµ–ä»»åŠ¡çš„å…¥åº¦
                for other_task_name in remaining_tasks:
                    if task.tool_name in normalized_dependencies.get(other_task_name, []):
                        in_degree[other_task_name] -= 1
            
            level_num += 1
        
        return levels
    
    def _optimize_parallel_execution(
        self,
        levels: List[ExecutionLevel],
        cached_results: Optional[Dict[str, Any]] = None
    ) -> List[ExecutionLevel]:
        """
        å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–
        
        Args:
            levels: æ‰§è¡Œå±‚çº§åˆ—è¡¨
            cached_results: ç¼“å­˜ç»“æœ
        
        Returns:
            List[ExecutionLevel]: ä¼˜åŒ–åçš„æ‰§è¡Œå±‚çº§åˆ—è¡¨
        """
        optimized_levels = []
        
        for level in levels:
            if not level.can_parallel:
                optimized_levels.append(level)
                continue
            
            # åˆ†æå¹¶è¡Œå®‰å…¨ç»„
            parallel_groups = self._group_parallel_safe_tasks(level.tasks, cached_results)
            level.parallel_groups = parallel_groups
            optimized_levels.append(level)
        
        return optimized_levels
    
    def _group_parallel_safe_tasks(
        self,
        tasks: List[DAGTask],
        cached_results: Optional[Dict[str, Any]] = None
    ) -> List[List[DAGTask]]:
        """
        åˆ†ç»„å¹¶è¡Œå®‰å…¨çš„ä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            cached_results: ç¼“å­˜ç»“æœ
        
        Returns:
            List[List[DAGTask]]: å¹¶è¡Œç»„åˆ—è¡¨
        """
        groups = []
        used_tasks = set()
        
        # ä¼˜å…ˆå¤„ç†ç¼“å­˜çš„ä»»åŠ¡
        if cached_results:
            cached_tasks = [task for task in tasks if task.tool_name in cached_results]
            if cached_tasks:
                groups.append(cached_tasks)
                used_tasks.update(task.tool_name for task in cached_tasks)
        
        # æŒ‰MCPæœåŠ¡å™¨åˆ†ç»„
        mcp_groups = defaultdict(list)
        for task in tasks:
            if task.tool_name not in used_tasks:
                mcp_groups[task.tool_metadata.mcp_server].append(task)
        
        # ä¸ºæ¯ä¸ªMCPæœåŠ¡å™¨åˆ›å»ºç»„
        for mcp_server, server_tasks in mcp_groups.items():
            if len(server_tasks) == 1:
                # å•ä¸ªä»»åŠ¡ç›´æ¥æˆç»„
                groups.append(server_tasks)
            else:
                # å¤šä¸ªä»»åŠ¡æŒ‰èµ„æºé™åˆ¶åˆ†ç»„
                if mcp_server in self.resource_pools:
                    semaphore = self.resource_pools[mcp_server]
                    max_concurrent = semaphore._value
                    for i in range(0, len(server_tasks), max_concurrent):
                        group = server_tasks[i:i + max_concurrent]
                        groups.append(group)
                else:
                    # æ²¡æœ‰èµ„æºé™åˆ¶ï¼Œå¯ä»¥å¹¶è¡Œ
                    groups.append(server_tasks)
        
        return groups
    
    async def _execute_levels(
        self,
        execution_id: str,
        levels: List[ExecutionLevel],
        context: Dict[str, Any],
        cached_results: Optional[Dict[str, Any]] = None
    ) -> DAGExecutionResult:
        """
        æ‰§è¡ŒDAGå±‚çº§
        
        Args:
            execution_id: æ‰§è¡ŒID
            levels: æ‰§è¡Œå±‚çº§åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            cached_results: ç¼“å­˜ç»“æœ
        
        Returns:
            DAGExecutionResult: æ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        all_results = (cached_results or {}).copy()
        all_errors = {}
        tasks_completed = 0
        tasks_failed = 0
        
        logger.info(f"ğŸ“Š æ‰§è¡ŒDAG: {len(levels)}ä¸ªå±‚çº§")
        
        for level_index, level in enumerate(levels):
            logger.info(f"ğŸ”„ æ‰§è¡Œå±‚çº§ {level_index + 1}/{len(levels)}: {len(level.tasks)}ä¸ªä»»åŠ¡")
            
            # æ‰§è¡Œå½“å‰å±‚çº§
            if level.parallel_groups:
                # å¹¶è¡Œæ‰§è¡Œç»„
                for group_index, group in enumerate(level.parallel_groups):
                    logger.info(f"  ğŸ“¦ æ‰§è¡Œå¹¶è¡Œç»„ {group_index + 1}/{len(level.parallel_groups)}: {len(group)}ä¸ªä»»åŠ¡")
                    
                    # å¹¶è¡Œæ‰§è¡Œç»„å†…ä»»åŠ¡
                    group_results = await asyncio.gather(
                        *[self._execute_single_task(task, execution_id, all_results) for task in group],
                        return_exceptions=True
                    )
                    
                    # æ”¶é›†ç»“æœ
                    for task, result in zip(group, group_results):
                        if isinstance(result, Exception):
                            all_errors[task.tool_name] = str(result)
                            tasks_failed += 1
                            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task.tool_name}, é”™è¯¯: {result}")
                        else:
                            all_results[task.tool_name] = result
                            tasks_completed += 1
                            logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task.tool_name}")
            
            else:
                # ä¸²è¡Œæ‰§è¡Œ
                for task in level.tasks:
                    try:
                        result = await self._execute_single_task(task, execution_id, all_results)
                        all_results[task.tool_name] = result
                        tasks_completed += 1
                        logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task.tool_name}")
                    except Exception as e:
                        all_errors[task.tool_name] = str(e)
                        tasks_failed += 1
                        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task.tool_name}, é”™è¯¯: {e}")
        
        total_time = time.time() - start_time
        
        return DAGExecutionResult(
            execution_id=execution_id,
            template_id=context.get("template_id", "unknown"),
            success=tasks_failed == 0,
            total_time=total_time,
            levels_executed=len(levels),
            tasks_completed=tasks_completed,
            tasks_failed=tasks_failed,
            results=all_results,
            errors=all_errors,
            performance_metrics={
                "total_tasks": tasks_completed + tasks_failed,
                "success_rate": tasks_completed / max(tasks_completed + tasks_failed, 1),
                "average_task_time": total_time / max(tasks_completed + tasks_failed, 1)
            }
        )
    
    async def _execute_single_task(
        self,
        task: DAGTask,
        execution_id: str,
        previous_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡
            execution_id: æ‰§è¡ŒID
            previous_results: ä¹‹å‰çš„ç»“æœ
        
        Returns:
            Dict: ä»»åŠ¡ç»“æœ
        """
        task.status = TaskStatus.RUNNING
        task.start_time = time.time()
        
        try:
            # è·å–èµ„æºé”
            mcp_server = task.tool_metadata.mcp_server
            if mcp_server in self.resource_pools:
                async with self.resource_pools[mcp_server]:
                    result = await self._execute_task_with_retry(task, previous_results)
            else:
                result = await self._execute_task_with_retry(task, previous_results)
            
            task.status = TaskStatus.COMPLETED
            task.end_time = time.time()
            
            return result
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.end_time = time.time()
            raise
    
    async def _execute_task_with_retry(
        self,
        task: DAGTask,
        previous_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¸¦é‡è¯•çš„ä»»åŠ¡æ‰§è¡Œ
        
        Args:
            task: ä»»åŠ¡
            previous_results: ä¹‹å‰çš„ç»“æœ
        
        Returns:
            Dict: ä»»åŠ¡ç»“æœ
        """
        max_retries = task.tool_metadata.retry_count
        
        for attempt in range(max_retries + 1):
            try:
                # æ£€æŸ¥ç¼“å­˜
                if task.tool_metadata.cacheable:
                    cached_result = await self._check_cache(task)
                    if cached_result:
                        logger.debug(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {task.tool_name}")
                        return cached_result
                
                # æ‰§è¡Œä»»åŠ¡
                result = await self._call_tool(task, previous_results)
                
                # ç¼“å­˜ç»“æœ
                if task.tool_metadata.cacheable:
                    await self._cache_result(task, result)
                
                return result
                
            except Exception as e:
                if attempt < max_retries:
                    task.retry_count = attempt + 1
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    logger.warning(f"âš ï¸ ä»»åŠ¡ {task.tool_name} æ‰§è¡Œå¤±è´¥ï¼Œ{wait_time}såé‡è¯• (ç¬¬{attempt + 1}æ¬¡)")
                    await asyncio.sleep(wait_time)
                else:
                    raise
        
        raise RuntimeError(f"ä»»åŠ¡ {task.tool_name} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    async def _check_cache(self, task: DAGTask) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥ç¼“å­˜"""
        if self.cache_manager:
            try:
                cache_key = self._generate_cache_key(task)
                cached_result = await self.cache_manager.get_cache(cache_key)
                if cached_result:
                    return cached_result
            except Exception as e:
                logger.warning(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {task.tool_name}, {e}")
        
        return None
    
    async def _cache_result(self, task: DAGTask, result: Dict[str, Any]):
        """ç¼“å­˜ç»“æœ"""
        if self.cache_manager and task.tool_metadata.cacheable:
            try:
                cache_key = self._generate_cache_key(task)
                await self.cache_manager.set_cache(
                    cache_key,
                    result,
                    ttl=task.tool_metadata.cache_ttl
                )
            except Exception as e:
                logger.warning(f"ç¼“å­˜å­˜å‚¨å¤±è´¥: {task.tool_name}, {e}")
    
    def _generate_cache_key(self, task: DAGTask) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            "tool_name": task.tool_name,
            "params": task.params,
            "user_id": task.params.get("user_id")
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    async def _call_tool(
        self,
        task: DAGTask,
        previous_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨å·¥å…·
        
        Args:
            task: ä»»åŠ¡
            previous_results: ä¹‹å‰çš„ç»“æœ
        
        Returns:
            Dict: å·¥å…·ç»“æœ
        """
        if self.tool_executor:
            # ä½¿ç”¨æ³¨å…¥çš„å·¥å…·æ‰§è¡Œå™¨
            return await self.tool_executor(
                task.tool_name,
                task.tool_metadata,
                task.params,
                previous_results
            )
        else:
            # é»˜è®¤å®ç°ï¼ˆè¿”å›æ¨¡æ‹Ÿç»“æœï¼‰
            logger.warning(f"âš ï¸ æœªæä¾›å·¥å…·æ‰§è¡Œå™¨ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ: {task.tool_name}")
            return {
                "tool": task.tool_name,
                "status": "success",
                "data": task.params,
                "timestamp": time.time(),
                "note": "æ¨¡æ‹Ÿç»“æœï¼ˆæœªæä¾›å·¥å…·æ‰§è¡Œå™¨ï¼‰"
            }
    
    def _generate_execution_id(self) -> str:
        """ç”Ÿæˆæ‰§è¡ŒID"""
        import uuid
        return f"dag_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    
    def _update_average_execution_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´"""
        total = self.performance_stats["successful_executions"]
        avg = self.performance_stats["average_execution_time"]
        self.performance_stats["average_execution_time"] = (
            (avg * (total - 1) + execution_time) / total
        )
    
    def get_performance_statistics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            **self.performance_stats,
            "execution_history": self.execution_history[-10:],  # æœ€è¿‘10æ¬¡æ‰§è¡Œ
            "tool_count": len(self.tool_registry)
        }
    
    def get_execution_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œå†å²"""
        return self.execution_history[-limit:]
