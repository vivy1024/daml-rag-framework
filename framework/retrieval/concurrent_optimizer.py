# -*- coding: utf-8 -*-
"""
å¹¶å‘æ‰§è¡Œä¼˜åŒ–å™¨ - Phase 2.3

ä¸“é—¨ä¼˜åŒ–Layer 1å’ŒLayer 2çš„å¹¶å‘æ‰§è¡Œæ€§èƒ½ã€‚

ä¼˜åŒ–ç­–ç•¥:
1. æ™ºèƒ½è¶…æ—¶æ§åˆ¶
2. ä¼˜å…ˆçº§ä»»åŠ¡è°ƒåº¦
3. è¿æ¥æ± å¤ç”¨
4. æ—©åœæœºåˆ¶
5. æ€§èƒ½é¢„æµ‹

ç‰ˆæœ¬: v2.0.0
æ—¥æœŸ: 2025-11-26
ä½œè€…: è–›å°å· (Phase 2)
"""

import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
import heapq
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    CRITICAL = 1    # å…³é”®ä»»åŠ¡
    HIGH = 2        # é«˜ä¼˜å…ˆçº§
    NORMAL = 3      # æ™®é€šä¼˜å…ˆçº§
    LOW = 4         # ä½ä¼˜å…ˆçº§


@dataclass
class ConcurrentTask:
    """å¹¶å‘ä»»åŠ¡"""
    task_id: str
    priority: TaskPriority
    coro: asyncio.Task
    start_time: float = field(default_factory=time.time)
    timeout: float = 10.0
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __lt__(self, other):
        """ä¼˜å…ˆçº§é˜Ÿåˆ—æ¯”è¾ƒ (æ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)"""
        return self.priority.value < other.priority.value


@dataclass
class OptimizationConfig:
    """å¹¶å‘ä¼˜åŒ–é…ç½®"""
    # è¶…æ—¶æ§åˆ¶
    layer1_timeout: float = 8.0      # Layer 1è¶…æ—¶æ—¶é—´
    layer2_timeout: float = 10.0     # Layer 2è¶…æ—¶æ—¶é—´
    total_timeout: float = 15.0      # æ€»è¶…æ—¶æ—¶é—´

    # ä»»åŠ¡è°ƒåº¦
    enable_priority_scheduling: bool = True
    max_concurrent_tasks: int = 2
    task_queue_size: int = 100

    # æ—©åœæœºåˆ¶
    enable_early_stopping: bool = True
    quality_threshold: float = 0.8   # è´¨é‡é˜ˆå€¼
    min_results_needed: int = 5      # æœ€å°‘éœ€è¦ç»“æœæ•°

    # æ€§èƒ½ä¼˜åŒ–
    enable_connection_pooling: bool = True
    reuse_http_sessions: bool = True
    max_retry_attempts: int = 3

    # ç›‘æ§
    enable_performance_tracking: bool = True
    detailed_metrics: bool = True


class PriorityTaskScheduler:
    """ä¼˜å…ˆçº§ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.task_queue = []
        self.running_tasks = {}
        self.completed_tasks = []
        self.performance_metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "timeout_tasks": 0,
            "avg_execution_time": 0.0
        }

    async def schedule_task(
        self,
        task_id: str,
        coro: Callable,
        priority: TaskPriority = TaskPriority.NORMAL,
        timeout: float = 10.0,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Any:
        """
        è°ƒåº¦ä»»åŠ¡æ‰§è¡Œ

        Args:
            task_id: ä»»åŠ¡ID
            coro: åç¨‹å‡½æ•°
            priority: ä¼˜å…ˆçº§
            timeout: è¶…æ—¶æ—¶é—´
            metadata: ä»»åŠ¡å…ƒæ•°æ®

        Returns:
            ä»»åŠ¡ç»“æœ
        """
        self.performance_metrics["total_tasks"] += 1

        try:
            # åˆ›å»ºä»»åŠ¡
            task = ConcurrentTask(
                task_id=task_id,
                priority=priority,
                coro=coro(),
                timeout=timeout,
                metadata=metadata or {}
            )

            logger.debug(f"ğŸ“‹ è°ƒåº¦ä»»åŠ¡: {task_id} (ä¼˜å…ˆçº§: {priority.name})")

            # ä½¿ç”¨asyncio.wait_forå®ç°è¶…æ—¶æ§åˆ¶
            result = await asyncio.wait_for(task.coro, timeout=timeout)

            self.performance_metrics["successful_tasks"] += 1

            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            execution_time = time.time() - task.start_time
            self._update_performance_metrics(execution_time, success=True)

            logger.debug(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id} (è€—æ—¶: {execution_time:.2f}s)")

            return result

        except asyncio.TimeoutError:
            self.performance_metrics["timeout_tasks"] += 1
            self.performance_metrics["failed_tasks"] += 1
            logger.warning(f"â° ä»»åŠ¡è¶…æ—¶: {task_id} (è¶…æ—¶: {timeout}s)")
            raise

        except Exception as e:
            self.performance_metrics["failed_tasks"] += 1
            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            raise

    def _update_performance_metrics(self, execution_time: float, success: bool):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        if success:
            # ç§»åŠ¨å¹³å‡
            alpha = 0.2
            self.performance_metrics["avg_execution_time"] = (
                self.performance_metrics["avg_execution_time"] * (1 - alpha) +
                execution_time * alpha
            )

    def get_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        metrics = self.performance_metrics.copy()

        if metrics["total_tasks"] > 0:
            metrics["success_rate"] = (
                metrics["successful_tasks"] / metrics["total_tasks"] * 100
            )
            metrics["timeout_rate"] = (
                metrics["timeout_tasks"] / metrics["total_tasks"] * 100
            )

        return metrics


class EarlyStoppingMonitor:
    """æ—©åœæœºåˆ¶ç›‘æ§å™¨"""

    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.start_time = time.time()
        self.best_results = []
        self.quality_history = []

    def should_stop_early(
        self,
        current_results: List[Dict[str, Any]],
        layer1_time: float,
        layer2_time: float
    ) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœ

        Args:
            current_results: å½“å‰ç»“æœåˆ—è¡¨
            layer1_time: Layer 1æ‰§è¡Œæ—¶é—´
            layer2_time: Layer 2æ‰§è¡Œæ—¶é—´

        Returns:
            (æ˜¯å¦æ—©åœ, æ—©åœåŸå› )
        """
        if not self.config.enable_early_stopping:
            return False, ""

        # æ£€æŸ¥æ€»è¶…æ—¶
        total_time = time.time() - self.start_time
        if total_time >= self.config.total_timeout:
            return True, f"æ€»è¶…æ—¶: {total_time:.2f}s >= {self.config.total_timeout}s"

        # æ£€æŸ¥ç»“æœè´¨é‡
        if current_results:
            avg_quality = sum(
                r.get("score", 0.0) for r in current_results
            ) / len(current_results)

            self.quality_history.append(avg_quality)

            # å¦‚æœè´¨é‡è¶³å¤Ÿå¥½ä¸”ç»“æœæ•°é‡å……è¶³
            if (avg_quality >= self.config.quality_threshold and
                len(current_results) >= self.config.min_results_needed):
                return True, f"è´¨é‡è¾¾æ ‡: {avg_quality:.2f} >= {self.config.quality_threshold}"

            # æ£€æŸ¥è´¨é‡æ”¹è¿›è¶‹åŠ¿
            if len(self.quality_history) >= 3:
                recent_avg = sum(self.quality_history[-3:]) / 3
                older_avg = sum(self.quality_history[:-3]) / len(self.quality_history[:-3])

                if recent_avg <= older_avg * 0.95:  # è´¨é‡ä¸‹é™è¶…è¿‡5%
                    return True, f"è´¨é‡ä¸‹é™: {recent_avg:.2f} < {older_avg:.2f}"

        # æ£€æŸ¥æ—¶é—´æ¯”ä¾‹
        if layer1_time > 0 and layer2_time > 0:
            time_ratio = max(layer1_time, layer2_time) / min(layer1_time, layer2_time)
            if time_ratio > 3.0:  # ä¸€å±‚æ˜æ˜¾æ…¢äºå¦ä¸€å±‚
                return True, f"æ—¶é—´ä¸å‡è¡¡: æ¯”ä¾‹ {time_ratio:.2f}"

        return False, ""


class ConnectionPoolManager:
    """è¿æ¥æ± ç®¡ç†å™¨"""

    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.http_sessions = {}
        self.neo4j_sessions = {}

    async def get_http_session(self, name: str) -> Any:
        """è·å–HTTPä¼šè¯"""
        if not self.config.enable_connection_pooling:
            import aiohttp
            return aiohttp.ClientSession()

        if name not in self.http_sessions:
            import aiohttp
            connector = aiohttp.TCPConnector(
                limit=10,
                limit_per_host=5,
                ttl_dns_cache=300
            )
            self.http_sessions[name] = aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(total=30)
            )

        return self.http_sessions[name]

    async def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        for session in self.http_sessions.values():
            await session.close()
        self.http_sessions.clear()

        for session in self.neo4j_sessions.values():
            session.close()
        self.neo4j_sessions.clear()


class ConcurrentOptimizer:
    """å¹¶å‘æ‰§è¡Œä¼˜åŒ–å™¨ - Phase 2.3æ ¸å¿ƒç»„ä»¶"""

    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.task_scheduler = PriorityTaskScheduler(config)
        self.early_stopping = EarlyStoppingMonitor(config)
        self.connection_manager = ConnectionPoolManager(config)

        logger.info(f"âœ… å¹¶å‘ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")
        logger.info(f"  - Layer1è¶…æ—¶: {config.layer1_timeout}s")
        logger.info(f"  - Layer2è¶…æ—¶: {config.layer2_timeout}s")
        logger.info(f"  - æ—©åœ: {config.enable_early_stopping}")
        logger.info(f"  - è¿æ¥æ± : {config.enable_connection_pooling}")

    async def execute_layer1_optimized(
        self,
        query: str,
        domain: str,
        top_k: int,
        filters: Optional[Dict[str, Any]],
        base_engine: Any
    ) -> Any:
        """ä¼˜åŒ–çš„Layer 1æ‰§è¡Œ"""
        logger.info("ğŸš€ æ‰§è¡Œä¼˜åŒ–çš„Layer 1: å‘é‡è¯­ä¹‰æ£€ç´¢")

        async def layer1_task():
            return await base_engine._execute_layer1_vector_search(
                query=query,
                domain=domain,
                top_k=top_k,
                filters=filters
            )

        return await self.task_scheduler.schedule_task(
            task_id="layer1_vector_search",
            coro=layer1_task,
            priority=TaskPriority.HIGH,
            timeout=self.config.layer1_timeout,
            metadata={"layer": "layer1", "type": "vector_search"}
        )

    async def execute_layer2_optimized(
        self,
        query: str,
        domain: str,
        top_k: int,
        base_engine: Any
    ) -> Any:
        """ä¼˜åŒ–çš„Layer 2æ‰§è¡Œ"""
        logger.info("ğŸš€ æ‰§è¡Œä¼˜åŒ–çš„Layer 2: å›¾è°±å…³ç³»æ¨ç†")

        async def layer2_task():
            return await base_engine._execute_layer2_graph_reasoning(
                query=query,
                domain=domain,
                vector_results=[],  # ä¼˜åŒ–ï¼šç‹¬ç«‹æ‰§è¡Œï¼Œä¸ä¾èµ–Layer1
                top_k=top_k
            )

        return await self.task_scheduler.schedule_task(
            task_id="layer2_graph_reasoning",
            coro=layer2_task,
            priority=TaskPriority.HIGH,
            timeout=self.config.layer2_timeout,
            metadata={"layer": "layer2", "type": "graph_reasoning"}
        )

    async def execute_parallel_optimized(
        self,
        query: str,
        domain: str,
        top_k: int,
        filters: Optional[Dict[str, Any]],
        base_engine: Any,
        user_profile: Optional[Dict[str, Any]] = None,
        safety_check: bool = True
    ) -> Tuple[Any, Any, Dict[str, Any]]:
        """
        æ‰§è¡Œä¼˜åŒ–çš„å¹¶è¡Œæ£€ç´¢

        Returns:
            (layer1_result, layer2_result, optimization_metrics)
        """
        start_time = time.time()

        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        logger.info(f"ğŸš€ å¹¶è¡Œä¼˜åŒ–æ£€ç´¢å¼€å§‹: {query[:50]}...")
        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        # åˆ›å»ºä»»åŠ¡
        layer1_coro = self.execute_layer1_optimized(
            query=query,
            domain=domain,
            top_k=top_k * 2,
            filters=filters,
            base_engine=base_engine
        )

        layer2_coro = self.execute_layer2_optimized(
            query=query,
            domain=domain,
            top_k=top_k * 2,
            base_engine=base_engine
        )

        # å¹¶å‘æ‰§è¡Œ (æ— ä¾èµ–å…³ç³»)
        layer1_task = asyncio.create_task(layer1_coro)
        layer2_task = asyncio.create_task(layer2_coro)

        # ç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–æ—©åœ
        layer1_result = None
        layer2_result = None
        layer1_time = 0.0
        layer2_time = 0.0

        try:
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
            layer1_result, layer2_result = await asyncio.gather(
                layer1_task,
                layer2_task,
                return_exceptions=True
            )

            # å¤„ç†å¼‚å¸¸
            if isinstance(layer1_result, Exception):
                logger.error(f"Layer1å¼‚å¸¸: {layer1_result}")
                layer1_result = None

            if isinstance(layer2_result, Exception):
                logger.error(f"Layer2å¼‚å¸¸: {layer2_result}")
                layer2_result = None

            # è®°å½•æ‰§è¡Œæ—¶é—´
            if layer1_result and hasattr(layer1_result, 'execution_time_ms'):
                layer1_time = layer1_result.execution_time_ms / 1000.0

            if layer2_result and hasattr(layer2_result, 'execution_time_ms'):
                layer2_time = layer2_result.execution_time_ms / 1000.0

            # æ£€æŸ¥æ—©åœ
            should_stop, stop_reason = self.early_stopping.should_stop_early(
                current_results=(layer1_result.results if layer1_result and layer1_result.success else []) +
                               (layer2_result.results if layer2_result and layer2_result.success else []),
                layer1_time=layer1_time,
                layer2_time=layer2_time
            )

            if should_stop:
                logger.info(f"â¹ï¸  æ—©åœè§¦å‘: {stop_reason}")

            # æ„å»ºä¼˜åŒ–æŒ‡æ ‡
            optimization_metrics = {
                "total_execution_time": time.time() - start_time,
                "layer1_time": layer1_time,
                "layer2_time": layer2_time,
                "parallel_efficiency": max(layer1_time, layer2_time) / (layer1_time + layer2_time) if (layer1_time + layer2_time) > 0 else 0,
                "early_stopping": should_stop,
                "early_stopping_reason": stop_reason,
                "task_scheduler_metrics": self.task_scheduler.get_metrics(),
                "connections_pooled": self.config.enable_connection_pooling,
                "quality_threshold": self.config.quality_threshold
            }

            logger.info(f"âœ… å¹¶è¡Œä¼˜åŒ–æ£€ç´¢å®Œæˆ")
            logger.info(f"  - Layer1: {len(layer1_result.results) if layer1_result and layer1_result.success else 0}ä¸ªç»“æœ")
            logger.info(f"  - Layer2: {len(layer2_result.results) if layer2_result and layer2_result.success else 0}ä¸ªç»“æœ")
            logger.info(f"  - å¹¶è¡Œæ•ˆç‡: {optimization_metrics['parallel_efficiency']:.2f}")
            logger.info(f"  - æ€»è€—æ—¶: {optimization_metrics['total_execution_time']:.2f}s")

            return layer1_result, layer2_result, optimization_metrics

        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œä¼˜åŒ–æ£€ç´¢å¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        await self.connection_manager.close_all()
        logger.info("å¹¶å‘ä¼˜åŒ–å™¨å·²å…³é—­")

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "task_scheduler": self.task_scheduler.get_metrics(),
            "early_stopping": {
                "enabled": self.config.enable_early_stopping,
                "quality_threshold": self.config.quality_threshold
            },
            "connection_pooling": {
                "enabled": self.config.enable_connection_pooling,
                "sessions_count": len(self.connection_manager.http_sessions)
            },
            "configuration": {
                "layer1_timeout": self.config.layer1_timeout,
                "layer2_timeout": self.config.layer2_timeout,
                "total_timeout": self.config.total_timeout,
                "max_concurrent_tasks": self.config.max_concurrent_tasks
            }
        }


__all__ = [
    "ConcurrentOptimizer",
    "OptimizationConfig",
    "PriorityTaskScheduler",
    "EarlyStoppingMonitor",
    "ConnectionPoolManager",
    "TaskPriority"
]
