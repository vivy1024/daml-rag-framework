# -*- coding: utf-8 -*-
"""å‘é‡æœç´¢å¼•æ“

æä¾›è¯­ä¹‰æœç´¢å’Œå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢åŠŸèƒ½
"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct,
    Filter, FieldCondition, MatchValue,
    SearchRequest
)
import hashlib

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    id: str
    score: float
    payload: Dict[str, Any]
    vector: Optional[np.ndarray] = None


class VectorSearchEngine:
    """
    å‘é‡æœç´¢å¼•æ“
    
    ç‰¹æ€§ï¼š
    - åŸºäºQdrantçš„é«˜æ€§èƒ½å‘é‡æœç´¢
    - æ”¯æŒå¤šç§embeddingæ¨¡å‹
    - æ‰¹é‡ç´¢å¼•å’Œæ£€ç´¢
    - è¿‡æ»¤å’Œæ··åˆæ£€ç´¢
    """
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6333,
        collection_name: str = "fitness_kg",
        vector_size: int = 1024,
        distance: str = "cosine",
        embedding_model: Optional[str] = "BAAI/bge-m3"
    ):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢å¼•æ“
        
        Args:
            host: QdrantæœåŠ¡å™¨åœ°å€
            port: Qdrantç«¯å£
            collection_name: é›†åˆåç§°
            vector_size: å‘é‡ç»´åº¦
            distance: è·ç¦»åº¦é‡ ("cosine", "euclidean", "dot")
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
        """
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.vector_size = vector_size
        
        # è¿æ¥Qdrant
        self.client = QdrantClient(host=host, port=port)
        
        # è·ç¦»åº¦é‡æ˜ å°„
        distance_map = {
            "cosine": Distance.COSINE,
            "euclidean": Distance.EUCLID,
            "dot": Distance.DOT
        }
        self.distance = distance_map.get(distance, Distance.COSINE)
        
        # åˆå§‹åŒ–é›†åˆ
        self._init_collection()
        
        # åµŒå…¥æ¨¡å‹
        self.embedding_model = embedding_model
        self.encoder = self._init_encoder(embedding_model)
        
        logger.info(
            f"âœ… VectorSearchEngineå·²åˆå§‹åŒ– "
            f"(collection={collection_name}, dim={vector_size})"
        )
    
    def _init_collection(self):
        """åˆå§‹åŒ–é›†åˆ"""
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.client.get_collections().collections
            exists = any(c.name == self.collection_name for c in collections)
            
            if not exists:
                # åˆ›å»ºæ–°é›†åˆ
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.vector_size,
                        distance=self.distance
                    )
                )
                logger.info(f"âœ… åˆ›å»ºQdranté›†åˆ: {self.collection_name}")
            else:
                logger.info(f"âœ… Qdranté›†åˆå·²å­˜åœ¨: {self.collection_name}")
        
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–Qdranté›†åˆå¤±è´¥: {e}")
            raise
    
    def _init_encoder(self, model_name: Optional[str]):
        """
        åˆå§‹åŒ–ç¼–ç å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        
        Returns:
            ç¼–ç å™¨å®ä¾‹
        """
        if not model_name:
            logger.warning("âš ï¸ æœªæŒ‡å®šembeddingæ¨¡å‹ï¼Œä½¿ç”¨éšæœºå‘é‡")
            return None
        
        try:
            # å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚BGEï¼‰
            if "bge" in model_name.lower():
                from sentence_transformers import SentenceTransformer
                encoder = SentenceTransformer(model_name)
                logger.info(f"âœ… åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
                return encoder
            
            # å°è¯•ä½¿ç”¨OpenAI
            elif "openai" in model_name.lower():
                import openai
                logger.info(f"âœ… ä½¿ç”¨OpenAIæ¨¡å‹: {model_name}")
                return openai
            
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹: {model_name}")
                return None
        
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½æ¨¡å‹ {model_name}: {e}")
            return None
    
    def encode(self, text: Union[str, List[str]]) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        Args:
            text: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            å‘é‡æˆ–å‘é‡æ•°ç»„
        """
        if self.encoder is None:
            # ä½¿ç”¨éšæœºå‘é‡ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
            if isinstance(text, str):
                return np.random.rand(self.vector_size).astype(np.float32)
            else:
                return np.random.rand(len(text), self.vector_size).astype(np.float32)
        
        # ä½¿ç”¨å®é™…æ¨¡å‹
        if hasattr(self.encoder, 'encode'):
            # SentenceTransformer
            vectors = self.encoder.encode(text, convert_to_numpy=True)
            return vectors.astype(np.float32)
        else:
            # OpenAIæˆ–å…¶ä»–
            logger.warning("âš ï¸ ç¼–ç å™¨æœªæ­£ç¡®é…ç½®")
            if isinstance(text, str):
                return np.random.rand(self.vector_size).astype(np.float32)
            else:
                return np.random.rand(len(text), self.vector_size).astype(np.float32)
    
    def add(
        self,
        id: str,
        vector: np.ndarray,
        payload: Dict[str, Any]
    ):
        """
        æ·»åŠ å•ä¸ªå‘é‡
        
        Args:
            id: å‘é‡ID
            vector: å‘é‡æ•°æ®
            payload: å…ƒæ•°æ®
        """
        point = PointStruct(
            id=self._hash_id(id),
            vector=vector.tolist(),
            payload=payload
        )
        
        self.client.upsert(
            collection_name=self.collection_name,
            points=[point]
        )
    
    def batch_add(
        self,
        ids: List[str],
        vectors: np.ndarray,
        payloads: List[Dict[str, Any]],
        batch_size: int = 100
    ):
        """
        æ‰¹é‡æ·»åŠ å‘é‡
        
        Args:
            ids: IDåˆ—è¡¨
            vectors: å‘é‡æ•°ç»„
            payloads: å…ƒæ•°æ®åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        total = len(ids)
        
        for i in range(0, total, batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_vectors = vectors[i:i + batch_size]
            batch_payloads = payloads[i:i + batch_size]
            
            points = [
                PointStruct(
                    id=self._hash_id(id_),
                    vector=vec.tolist(),
                    payload=payload
                )
                for id_, vec, payload in zip(batch_ids, batch_vectors, batch_payloads)
            ]
            
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            logger.info(f"  æ‰¹æ¬¡ {i//batch_size + 1}: æ·»åŠ äº† {len(points)} ä¸ªå‘é‡")
        
        logger.info(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ: æ€»å…± {total} ä¸ªå‘é‡")
    
    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        min_similarity: Optional[float] = None,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """
        å‘é‡æœç´¢

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            filter: è¿‡æ»¤æ¡ä»¶ï¼ˆå‘åå…¼å®¹ï¼‰
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            filters: è¿‡æ»¤æ¡ä»¶ï¼ˆæ–°ç‰ˆæœ¬ï¼‰

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # æ„å»ºè¿‡æ»¤å™¨ï¼ˆæ”¯æŒæ–°æ—§å‚æ•°ï¼‰
        filter_param = filters or filter
        qdrant_filter = None
        if filter_param:
            qdrant_filter = self._build_filter(filter_param)

        # æ‰§è¡Œæœç´¢ï¼ˆä½¿ç”¨query_pointsæ–¹æ³•ï¼‰
        results = self.client.query_points(
            collection_name=self.collection_name,
            query=query_vector.tolist(),
            limit=top_k,
            query_filter=qdrant_filter
        )

        # è½¬æ¢ç»“æœå¹¶åº”ç”¨ç›¸ä¼¼åº¦è¿‡æ»¤
        search_results = []
        # query_pointsè¿”å›ScoredPointå¯¹è±¡ï¼Œéœ€è¦ä»pointså±æ€§è·å–
        if hasattr(results, 'points'):
            points = results.points
        else:
            points = results

        for r in points:
            # å¤„ç†ä¸åŒçš„ç»“æœæ ¼å¼
            if hasattr(r, 'score'):
                score = r.score
                point_id = r.id
                payload = r.payload if hasattr(r, 'payload') else {}
            else:
                # å¦‚æœæ˜¯tupleæ ¼å¼ (id, score, payload)
                if len(r) >= 3:
                    point_id, score, payload = r[0], r[1], r[2]
                elif len(r) == 2:
                    point_id, score = r
                    payload = {}
                else:
                    continue

            # åº”ç”¨æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            if min_similarity is not None and score < min_similarity:
                continue
            search_results.append(SearchResult(
                id=str(point_id),
                score=score,
                payload=payload
            ))

        return search_results
    
    def search_by_text(
        self,
        query: str,
        top_k: int = 10,
        filter: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """
        æ–‡æœ¬è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filter: è¿‡æ»¤æ¡ä»¶
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.encode(query)
        
        # æ‰§è¡Œå‘é‡æœç´¢
        return self.search(query_vector, top_k, filter)
    
    def batch_search(
        self,
        query_vectors: np.ndarray,
        top_k: int = 10
    ) -> List[List[SearchResult]]:
        """
        æ‰¹é‡æœç´¢
        
        Args:
            query_vectors: æŸ¥è¯¢å‘é‡æ•°ç»„
            top_k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
        
        Returns:
            æ¯ä¸ªæŸ¥è¯¢çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        for query_vector in query_vectors:
            search_results = self.search(query_vector, top_k)
            results.append(search_results)
        
        return results
    
    def get_by_id(self, id: str) -> Optional[SearchResult]:
        """
        æ ¹æ®IDè·å–å‘é‡
        
        Args:
            id: å‘é‡ID
        
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            point = self.client.retrieve(
                collection_name=self.collection_name,
                ids=[self._hash_id(id)]
            )
            
            if point:
                p = point[0]
                return SearchResult(
                    id=str(p.id),
                    score=1.0,
                    payload=p.payload,
                    vector=np.array(p.vector) if p.vector else None
                )
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–å‘é‡å¤±è´¥ (id={id}): {e}")
        
        return None
    
    def delete(self, id: str):
        """
        åˆ é™¤å‘é‡
        
        Args:
            id: å‘é‡ID
        """
        self.client.delete(
            collection_name=self.collection_name,
            points_selector=[self._hash_id(id)]
        )
    
    def count(self) -> int:
        """
        è·å–å‘é‡æ•°é‡
        
        Returns:
            å‘é‡æ€»æ•°
        """
        info = self.client.get_collection(self.collection_name)
        return info.points_count
    
    def _hash_id(self, id: str) -> int:
        """
        å°†å­—ç¬¦ä¸²IDå“ˆå¸Œä¸ºæ•´æ•°
        
        Args:
            id: å­—ç¬¦ä¸²ID
        
        Returns:
            æ•´æ•°ID
        """
        return int(hashlib.md5(id.encode()).hexdigest()[:16], 16) % (10 ** 12)
    
    def _build_filter(self, filter_dict: Dict[str, Any]) -> Filter:
        """
        æ„å»ºQdrantè¿‡æ»¤å™¨
        
        Args:
            filter_dict: è¿‡æ»¤å­—å…¸ {"field": "value"}
        
        Returns:
            Qdrantè¿‡æ»¤å™¨
        """
        conditions = [
            FieldCondition(
                key=key,
                match=MatchValue(value=value)
            )
            for key, value in filter_dict.items()
        ]
        
        return Filter(must=conditions)
    
    def clear(self):
        """æ¸…ç©ºé›†åˆ"""
        try:
            self.client.delete_collection(self.collection_name)
            self._init_collection()
            logger.info(f"âœ… é›†åˆå·²æ¸…ç©º: {self.collection_name}")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºé›†åˆå¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()
            logger.info("ğŸ”’ Qdrantè¿æ¥å·²å…³é—­")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()


# ==================== è¾…åŠ©å‡½æ•° ====================

def create_text_for_embedding(node: Dict[str, Any]) -> str:
    """
    ä¸ºèŠ‚ç‚¹åˆ›å»ºç”¨äºembeddingçš„æ–‡æœ¬
    
    Args:
        node: èŠ‚ç‚¹æ•°æ®
    
    Returns:
        æ–‡æœ¬å­—ç¬¦ä¸²
    """
    parts = []
    
    # æ·»åŠ åç§°
    if "name_zh" in node:
        parts.append(node["name_zh"])
    elif "name" in node:
        parts.append(node["name"])
    
    # æ·»åŠ æè¿°
    if "description" in node:
        parts.append(node["description"])
    
    # æ·»åŠ å…¶ä»–é‡è¦å­—æ®µ
    for key in ["category", "type", "difficulty", "equipment"]:
        if key in node and node[key]:
            parts.append(str(node[key]))
    
    return " ".join(parts)

