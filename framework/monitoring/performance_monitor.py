# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§ç³»ç»Ÿ v1.0 - DAGæ‰§è¡Œæ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

æä¾›å…¨é¢çš„æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. DAGæ‰§è¡Œæ€§èƒ½ç›‘æ§
2. LLMè°ƒç”¨ç»Ÿè®¡
3. ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡
4. å·¥å…·æ‰§è¡Œæ—¶é—´åˆ†æ
5. æ€§èƒ½è¶‹åŠ¿åˆ†æ

ä½œè€…: BUILD_BODY Team
ç‰ˆæœ¬: v1.0.0
æ—¥æœŸ: 2025-12-12
"""

import logging
import time
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
from datetime import datetime, timedelta
import statistics

logger = logging.getLogger(__name__)


@dataclass
class ToolExecutionMetrics:
    """å·¥å…·æ‰§è¡ŒæŒ‡æ ‡"""
    tool_name: str
    execution_id: str
    user_id: str
    start_time: float
    end_time: float
    duration: float
    success: bool
    error_message: Optional[str] = None
    cache_hit: bool = False
    retry_count: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DAGExecutionMetrics:
    """DAGæ‰§è¡ŒæŒ‡æ ‡"""
    execution_id: str
    template_id: str
    template_name: str
    user_id: str
    start_time: float
    end_time: float
    total_duration: float
    tools_executed: int
    tools_succeeded: int
    tools_failed: int
    tools_cached: int
    parallel_groups: int
    max_parallel_degree: int
    cache_hit_rate: float
    success_rate: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LLMCallMetrics:
    """LLMè°ƒç”¨æŒ‡æ ‡"""
    call_id: str
    call_type: str  # "decision" or "analysis"
    model_name: str
    user_id: str
    start_time: float
    end_time: float
    duration: float
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    success: bool
    error_message: Optional[str] = None
    confidence: float = 0.0
    fallback_used: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class CacheMetrics:
    """ç¼“å­˜æŒ‡æ ‡"""
    timestamp: float
    hits: int
    misses: int
    hit_rate: float
    preloads: int
    evictions: int
    total_size: int
    memory_cache_size: int
    tool_specific_stats: Dict[str, Dict[str, int]] = field(default_factory=dict)


@dataclass
class PerformanceSnapshot:
    """æ€§èƒ½å¿«ç…§"""
    timestamp: float
    period_seconds: int
    dag_executions: int
    avg_dag_duration: float
    llm_calls: int
    avg_llm_duration: float
    cache_hit_rate: float
    success_rate: float
    top_tools: List[Tuple[str, int]]
    top_errors: List[Tuple[str, int]]


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, max_history_size: int = 1000):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
        
        Args:
            max_history_size: æœ€å¤§å†å²è®°å½•æ•°é‡
        """
        self.max_history_size = max_history_size
        
        # å·¥å…·æ‰§è¡Œå†å²
        self.tool_executions: deque = deque(maxlen=max_history_size)
        
        # DAGæ‰§è¡Œå†å²
        self.dag_executions: deque = deque(maxlen=max_history_size)
        
        # LLMè°ƒç”¨å†å²
        self.llm_calls: deque = deque(maxlen=max_history_size)
        
        # ç¼“å­˜æŒ‡æ ‡å†å²
        self.cache_metrics_history: deque = deque(maxlen=100)
        
        # å®æ—¶ç»Ÿè®¡
        self.tool_stats = defaultdict(lambda: {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "total_duration": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        })
        
        self.llm_stats = defaultdict(lambda: {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "total_duration": 0.0,
            "total_tokens": 0,
            "fallback_count": 0
        })
        
        # é”™è¯¯ç»Ÿè®¡
        self.error_counts = defaultdict(int)
        
        logger.info("âœ… æ€§èƒ½ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    # ========== å·¥å…·æ‰§è¡Œç›‘æ§ ==========
    
    def record_tool_execution(self, metrics: ToolExecutionMetrics):
        """
        è®°å½•å·¥å…·æ‰§è¡ŒæŒ‡æ ‡
        
        Args:
            metrics: å·¥å…·æ‰§è¡ŒæŒ‡æ ‡
        """
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.tool_executions.append(metrics)
        
        # æ›´æ–°å®æ—¶ç»Ÿè®¡
        stats = self.tool_stats[metrics.tool_name]
        stats["total_calls"] += 1
        
        if metrics.success:
            stats["successful_calls"] += 1
        else:
            stats["failed_calls"] += 1
            if metrics.error_message:
                self.error_counts[metrics.error_message] += 1
        
        stats["total_duration"] += metrics.duration
        
        if metrics.cache_hit:
            stats["cache_hits"] += 1
        else:
            stats["cache_misses"] += 1
        
        logger.debug(
            f"ğŸ“Š è®°å½•å·¥å…·æ‰§è¡Œ: {metrics.tool_name}, "
            f"è€—æ—¶: {metrics.duration:.2f}s, "
            f"æˆåŠŸ: {metrics.success}"
        )
    
    def get_tool_statistics(
        self,
        tool_name: Optional[str] = None,
        time_window_seconds: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è·å–å·¥å…·ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            tool_name: å·¥å…·åç§°ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ‰€æœ‰å·¥å…·ï¼‰
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            Dict[str, Any]: å·¥å…·ç»Ÿè®¡ä¿¡æ¯
        """
        # è¿‡æ»¤æ—¶é—´çª—å£
        if time_window_seconds:
            cutoff_time = time.time() - time_window_seconds
            filtered_executions = [
                m for m in self.tool_executions
                if m.start_time >= cutoff_time
            ]
        else:
            filtered_executions = list(self.tool_executions)
        
        # è¿‡æ»¤å·¥å…·åç§°
        if tool_name:
            filtered_executions = [
                m for m in filtered_executions
                if m.tool_name == tool_name
            ]
        
        if not filtered_executions:
            return {
                "tool_name": tool_name,
                "total_calls": 0,
                "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ‰§è¡Œè®°å½•"
            }
        
        # è®¡ç®—ç»Ÿè®¡
        total_calls = len(filtered_executions)
        successful_calls = sum(1 for m in filtered_executions if m.success)
        failed_calls = total_calls - successful_calls
        cache_hits = sum(1 for m in filtered_executions if m.cache_hit)
        
        durations = [m.duration for m in filtered_executions]
        
        return {
            "tool_name": tool_name or "all",
            "total_calls": total_calls,
            "successful_calls": successful_calls,
            "failed_calls": failed_calls,
            "success_rate": successful_calls / total_calls if total_calls > 0 else 0.0,
            "cache_hits": cache_hits,
            "cache_misses": total_calls - cache_hits,
            "cache_hit_rate": cache_hits / total_calls if total_calls > 0 else 0.0,
            "duration_stats": {
                "min": min(durations) if durations else 0.0,
                "max": max(durations) if durations else 0.0,
                "avg": statistics.mean(durations) if durations else 0.0,
                "median": statistics.median(durations) if durations else 0.0,
                "p95": self._calculate_percentile(durations, 0.95) if durations else 0.0,
                "p99": self._calculate_percentile(durations, 0.99) if durations else 0.0
            },
            "time_window_seconds": time_window_seconds
        }
    
    # ========== DAGæ‰§è¡Œç›‘æ§ ==========
    
    def record_dag_execution(self, metrics: DAGExecutionMetrics):
        """
        è®°å½•DAGæ‰§è¡ŒæŒ‡æ ‡
        
        Args:
            metrics: DAGæ‰§è¡ŒæŒ‡æ ‡
        """
        self.dag_executions.append(metrics)
        
        logger.debug(
            f"ğŸ“Š è®°å½•DAGæ‰§è¡Œ: {metrics.template_name}, "
            f"è€—æ—¶: {metrics.total_duration:.2f}s, "
            f"æˆåŠŸç‡: {metrics.success_rate:.1%}"
        )
    
    def get_dag_statistics(
        self,
        template_id: Optional[str] = None,
        time_window_seconds: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è·å–DAGç»Ÿè®¡ä¿¡æ¯
        
        Args:
            template_id: æ¨¡æ¿IDï¼ˆå¯é€‰ï¼‰
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            Dict[str, Any]: DAGç»Ÿè®¡ä¿¡æ¯
        """
        # è¿‡æ»¤æ—¶é—´çª—å£
        if time_window_seconds:
            cutoff_time = time.time() - time_window_seconds
            filtered_executions = [
                m for m in self.dag_executions
                if m.start_time >= cutoff_time
            ]
        else:
            filtered_executions = list(self.dag_executions)
        
        # è¿‡æ»¤æ¨¡æ¿ID
        if template_id:
            filtered_executions = [
                m for m in filtered_executions
                if m.template_id == template_id
            ]
        
        if not filtered_executions:
            return {
                "template_id": template_id,
                "total_executions": 0,
                "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ‰§è¡Œè®°å½•"
            }
        
        # è®¡ç®—ç»Ÿè®¡
        total_executions = len(filtered_executions)
        durations = [m.total_duration for m in filtered_executions]
        success_rates = [m.success_rate for m in filtered_executions]
        cache_hit_rates = [m.cache_hit_rate for m in filtered_executions]
        
        return {
            "template_id": template_id or "all",
            "total_executions": total_executions,
            "duration_stats": {
                "min": min(durations) if durations else 0.0,
                "max": max(durations) if durations else 0.0,
                "avg": statistics.mean(durations) if durations else 0.0,
                "median": statistics.median(durations) if durations else 0.0,
                "p95": self._calculate_percentile(durations, 0.95) if durations else 0.0
            },
            "success_rate": {
                "avg": statistics.mean(success_rates) if success_rates else 0.0,
                "min": min(success_rates) if success_rates else 0.0,
                "max": max(success_rates) if success_rates else 0.0
            },
            "cache_hit_rate": {
                "avg": statistics.mean(cache_hit_rates) if cache_hit_rates else 0.0,
                "min": min(cache_hit_rates) if cache_hit_rates else 0.0,
                "max": max(cache_hit_rates) if cache_hit_rates else 0.0
            },
            "avg_tools_executed": statistics.mean([m.tools_executed for m in filtered_executions]) if filtered_executions else 0.0,
            "avg_parallel_groups": statistics.mean([m.parallel_groups for m in filtered_executions]) if filtered_executions else 0.0,
            "time_window_seconds": time_window_seconds
        }
    
    # ========== LLMè°ƒç”¨ç›‘æ§ ==========
    
    def record_llm_call(self, metrics: LLMCallMetrics):
        """
        è®°å½•LLMè°ƒç”¨æŒ‡æ ‡
        
        Args:
            metrics: LLMè°ƒç”¨æŒ‡æ ‡
        """
        self.llm_calls.append(metrics)
        
        # æ›´æ–°å®æ—¶ç»Ÿè®¡
        stats = self.llm_stats[metrics.call_type]
        stats["total_calls"] += 1
        
        if metrics.success:
            stats["successful_calls"] += 1
        else:
            stats["failed_calls"] += 1
        
        stats["total_duration"] += metrics.duration
        stats["total_tokens"] += metrics.total_tokens
        
        if metrics.fallback_used:
            stats["fallback_count"] += 1
        
        logger.debug(
            f"ğŸ“Š è®°å½•LLMè°ƒç”¨: {metrics.call_type}, "
            f"è€—æ—¶: {metrics.duration:.2f}s, "
            f"tokens: {metrics.total_tokens}"
        )
    
    def get_llm_statistics(
        self,
        call_type: Optional[str] = None,
        time_window_seconds: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è·å–LLMç»Ÿè®¡ä¿¡æ¯
        
        Args:
            call_type: è°ƒç”¨ç±»å‹ï¼ˆå¯é€‰ï¼‰
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            Dict[str, Any]: LLMç»Ÿè®¡ä¿¡æ¯
        """
        # è¿‡æ»¤æ—¶é—´çª—å£
        if time_window_seconds:
            cutoff_time = time.time() - time_window_seconds
            filtered_calls = [
                m for m in self.llm_calls
                if m.start_time >= cutoff_time
            ]
        else:
            filtered_calls = list(self.llm_calls)
        
        # è¿‡æ»¤è°ƒç”¨ç±»å‹
        if call_type:
            filtered_calls = [
                m for m in filtered_calls
                if m.call_type == call_type
            ]
        
        if not filtered_calls:
            return {
                "call_type": call_type,
                "total_calls": 0,
                "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è°ƒç”¨è®°å½•"
            }
        
        # è®¡ç®—ç»Ÿè®¡
        total_calls = len(filtered_calls)
        successful_calls = sum(1 for m in filtered_calls if m.success)
        fallback_calls = sum(1 for m in filtered_calls if m.fallback_used)
        
        durations = [m.duration for m in filtered_calls]
        total_tokens = sum(m.total_tokens for m in filtered_calls)
        prompt_tokens = sum(m.prompt_tokens for m in filtered_calls)
        completion_tokens = sum(m.completion_tokens for m in filtered_calls)
        
        return {
            "call_type": call_type or "all",
            "total_calls": total_calls,
            "successful_calls": successful_calls,
            "failed_calls": total_calls - successful_calls,
            "success_rate": successful_calls / total_calls if total_calls > 0 else 0.0,
            "fallback_calls": fallback_calls,
            "fallback_rate": fallback_calls / total_calls if total_calls > 0 else 0.0,
            "duration_stats": {
                "min": min(durations) if durations else 0.0,
                "max": max(durations) if durations else 0.0,
                "avg": statistics.mean(durations) if durations else 0.0,
                "median": statistics.median(durations) if durations else 0.0,
                "p95": self._calculate_percentile(durations, 0.95) if durations else 0.0
            },
            "token_stats": {
                "total_tokens": total_tokens,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "avg_tokens_per_call": total_tokens / total_calls if total_calls > 0 else 0.0
            },
            "time_window_seconds": time_window_seconds
        }
    
    # ========== ç¼“å­˜ç›‘æ§ ==========
    
    def record_cache_metrics(self, metrics: CacheMetrics):
        """
        è®°å½•ç¼“å­˜æŒ‡æ ‡
        
        Args:
            metrics: ç¼“å­˜æŒ‡æ ‡
        """
        self.cache_metrics_history.append(metrics)
        
        logger.debug(
            f"ğŸ“Š è®°å½•ç¼“å­˜æŒ‡æ ‡: å‘½ä¸­ç‡={metrics.hit_rate:.1%}, "
            f"é¢„åŠ è½½={metrics.preloads}, æ·˜æ±°={metrics.evictions}"
        )
    
    def get_cache_statistics(
        self,
        time_window_seconds: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            Dict[str, Any]: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        # è¿‡æ»¤æ—¶é—´çª—å£
        if time_window_seconds:
            cutoff_time = time.time() - time_window_seconds
            filtered_metrics = [
                m for m in self.cache_metrics_history
                if m.timestamp >= cutoff_time
            ]
        else:
            filtered_metrics = list(self.cache_metrics_history)
        
        if not filtered_metrics:
            return {
                "message": "æ²¡æœ‰ç¼“å­˜æŒ‡æ ‡è®°å½•"
            }
        
        # è·å–æœ€æ–°æŒ‡æ ‡
        latest_metrics = filtered_metrics[-1]
        
        # è®¡ç®—è¶‹åŠ¿
        hit_rates = [m.hit_rate for m in filtered_metrics]
        
        return {
            "current": {
                "hits": latest_metrics.hits,
                "misses": latest_metrics.misses,
                "hit_rate": latest_metrics.hit_rate,
                "preloads": latest_metrics.preloads,
                "evictions": latest_metrics.evictions,
                "total_size": latest_metrics.total_size,
                "memory_cache_size": latest_metrics.memory_cache_size
            },
            "trends": {
                "avg_hit_rate": statistics.mean(hit_rates) if hit_rates else 0.0,
                "min_hit_rate": min(hit_rates) if hit_rates else 0.0,
                "max_hit_rate": max(hit_rates) if hit_rates else 0.0,
                "total_preloads": sum(m.preloads for m in filtered_metrics),
                "total_evictions": sum(m.evictions for m in filtered_metrics)
            },
            "tool_specific": latest_metrics.tool_specific_stats,
            "time_window_seconds": time_window_seconds
        }
    
    # ========== ç»¼åˆç»Ÿè®¡å’Œåˆ†æ ==========
    
    def get_performance_snapshot(
        self,
        time_window_seconds: int = 3600
    ) -> PerformanceSnapshot:
        """
        è·å–æ€§èƒ½å¿«ç…§
        
        Args:
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
            
        Returns:
            PerformanceSnapshot: æ€§èƒ½å¿«ç…§
        """
        cutoff_time = time.time() - time_window_seconds
        
        # è¿‡æ»¤DAGæ‰§è¡Œ
        recent_dag_executions = [
            m for m in self.dag_executions
            if m.start_time >= cutoff_time
        ]
        
        # è¿‡æ»¤LLMè°ƒç”¨
        recent_llm_calls = [
            m for m in self.llm_calls
            if m.start_time >= cutoff_time
        ]
        
        # è¿‡æ»¤å·¥å…·æ‰§è¡Œ
        recent_tool_executions = [
            m for m in self.tool_executions
            if m.start_time >= cutoff_time
        ]
        
        # è®¡ç®—å¹³å‡DAGæ‰§è¡Œæ—¶é—´
        avg_dag_duration = (
            statistics.mean([m.total_duration for m in recent_dag_executions])
            if recent_dag_executions else 0.0
        )
        
        # è®¡ç®—å¹³å‡LLMè°ƒç”¨æ—¶é—´
        avg_llm_duration = (
            statistics.mean([m.duration for m in recent_llm_calls])
            if recent_llm_calls else 0.0
        )
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        cache_hits = sum(1 for m in recent_tool_executions if m.cache_hit)
        total_tool_calls = len(recent_tool_executions)
        cache_hit_rate = cache_hits / total_tool_calls if total_tool_calls > 0 else 0.0
        
        # è®¡ç®—æˆåŠŸç‡
        successful_dag = sum(1 for m in recent_dag_executions if m.success_rate > 0.9)
        success_rate = successful_dag / len(recent_dag_executions) if recent_dag_executions else 0.0
        
        # ç»Ÿè®¡æœ€å¸¸ç”¨å·¥å…·
        tool_counts = defaultdict(int)
        for m in recent_tool_executions:
            tool_counts[m.tool_name] += 1
        top_tools = sorted(tool_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # ç»Ÿè®¡æœ€å¸¸è§é”™è¯¯
        error_counts_recent = defaultdict(int)
        for m in recent_tool_executions:
            if not m.success and m.error_message:
                error_counts_recent[m.error_message] += 1
        top_errors = sorted(error_counts_recent.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return PerformanceSnapshot(
            timestamp=time.time(),
            period_seconds=time_window_seconds,
            dag_executions=len(recent_dag_executions),
            avg_dag_duration=avg_dag_duration,
            llm_calls=len(recent_llm_calls),
            avg_llm_duration=avg_llm_duration,
            cache_hit_rate=cache_hit_rate,
            success_rate=success_rate,
            top_tools=top_tools,
            top_errors=top_errors
        )
    
    def get_performance_trends(
        self,
        time_window_seconds: int = 86400,  # 24å°æ—¶
        interval_seconds: int = 3600  # 1å°æ—¶é—´éš”
    ) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½è¶‹åŠ¿åˆ†æ
        
        Args:
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            interval_seconds: é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            Dict[str, Any]: æ€§èƒ½è¶‹åŠ¿æ•°æ®
        """
        current_time = time.time()
        start_time = current_time - time_window_seconds
        
        # ç”Ÿæˆæ—¶é—´ç‚¹
        time_points = []
        t = start_time
        while t <= current_time:
            time_points.append(t)
            t += interval_seconds
        
        # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹è®¡ç®—æŒ‡æ ‡
        trends = {
            "time_points": [datetime.fromtimestamp(t).isoformat() for t in time_points],
            "dag_execution_counts": [],
            "avg_dag_durations": [],
            "llm_call_counts": [],
            "avg_llm_durations": [],
            "cache_hit_rates": [],
            "success_rates": []
        }
        
        for i in range(len(time_points) - 1):
            interval_start = time_points[i]
            interval_end = time_points[i + 1]
            
            # è¿‡æ»¤è¯¥æ—¶é—´æ®µçš„æ•°æ®
            interval_dag_executions = [
                m for m in self.dag_executions
                if interval_start <= m.start_time < interval_end
            ]
            
            interval_llm_calls = [
                m for m in self.llm_calls
                if interval_start <= m.start_time < interval_end
            ]
            
            interval_tool_executions = [
                m for m in self.tool_executions
                if interval_start <= m.start_time < interval_end
            ]
            
            # è®¡ç®—æŒ‡æ ‡
            trends["dag_execution_counts"].append(len(interval_dag_executions))
            trends["avg_dag_durations"].append(
                statistics.mean([m.total_duration for m in interval_dag_executions])
                if interval_dag_executions else 0.0
            )
            
            trends["llm_call_counts"].append(len(interval_llm_calls))
            trends["avg_llm_durations"].append(
                statistics.mean([m.duration for m in interval_llm_calls])
                if interval_llm_calls else 0.0
            )
            
            cache_hits = sum(1 for m in interval_tool_executions if m.cache_hit)
            total_calls = len(interval_tool_executions)
            trends["cache_hit_rates"].append(
                cache_hits / total_calls if total_calls > 0 else 0.0
            )
            
            successful = sum(1 for m in interval_dag_executions if m.success_rate > 0.9)
            trends["success_rates"].append(
                successful / len(interval_dag_executions)
                if interval_dag_executions else 0.0
            )
        
        return trends
    
    def get_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """
        è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®
        
        Returns:
            List[Dict[str, Any]]: ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        recommendations = []
        
        # åˆ†æå·¥å…·æ€§èƒ½
        for tool_name, stats in self.tool_stats.items():
            if stats["total_calls"] < 10:
                continue
            
            avg_duration = stats["total_duration"] / stats["total_calls"]
            cache_hit_rate = (
                stats["cache_hits"] / (stats["cache_hits"] + stats["cache_misses"])
                if (stats["cache_hits"] + stats["cache_misses"]) > 0 else 0.0
            )
            
            # æ…¢å·¥å…·å»ºè®®
            if avg_duration > 2.0:
                recommendations.append({
                    "type": "slow_tool",
                    "severity": "high",
                    "tool_name": tool_name,
                    "avg_duration": avg_duration,
                    "recommendation": f"å·¥å…· {tool_name} å¹³å‡æ‰§è¡Œæ—¶é—´ {avg_duration:.2f}sï¼Œå»ºè®®ä¼˜åŒ–æˆ–å¢åŠ ç¼“å­˜"
                })
            
            # ä½ç¼“å­˜å‘½ä¸­ç‡å»ºè®®
            if cache_hit_rate < 0.3 and stats["total_calls"] > 20:
                recommendations.append({
                    "type": "low_cache_hit_rate",
                    "severity": "medium",
                    "tool_name": tool_name,
                    "cache_hit_rate": cache_hit_rate,
                    "recommendation": f"å·¥å…· {tool_name} ç¼“å­˜å‘½ä¸­ç‡ä»… {cache_hit_rate:.1%}ï¼Œå»ºè®®å¢åŠ ç¼“å­˜TTLæˆ–é¢„åŠ è½½"
                })
            
            # é«˜å¤±è´¥ç‡å»ºè®®
            failure_rate = stats["failed_calls"] / stats["total_calls"]
            if failure_rate > 0.1:
                recommendations.append({
                    "type": "high_failure_rate",
                    "severity": "high",
                    "tool_name": tool_name,
                    "failure_rate": failure_rate,
                    "recommendation": f"å·¥å…· {tool_name} å¤±è´¥ç‡ {failure_rate:.1%}ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯æ—¥å¿—å¹¶ä¿®å¤"
                })
        
        # åˆ†æLLMæ€§èƒ½
        for call_type, stats in self.llm_stats.items():
            if stats["total_calls"] < 5:
                continue
            
            avg_duration = stats["total_duration"] / stats["total_calls"]
            fallback_rate = stats["fallback_count"] / stats["total_calls"]
            
            # LLMæ…¢è°ƒç”¨å»ºè®®
            if avg_duration > 5.0:
                recommendations.append({
                    "type": "slow_llm_call",
                    "severity": "medium",
                    "call_type": call_type,
                    "avg_duration": avg_duration,
                    "recommendation": f"LLM {call_type} è°ƒç”¨å¹³å‡è€—æ—¶ {avg_duration:.2f}sï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹"
                })
            
            # é«˜é™çº§ç‡å»ºè®®
            if fallback_rate > 0.2:
                recommendations.append({
                    "type": "high_fallback_rate",
                    "severity": "high",
                    "call_type": call_type,
                    "fallback_rate": fallback_rate,
                    "recommendation": f"LLM {call_type} é™çº§ç‡ {fallback_rate:.1%}ï¼Œå»ºè®®æ£€æŸ¥LLMæœåŠ¡ç¨³å®šæ€§"
                })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {"high": 0, "medium": 1, "low": 2}
        recommendations.sort(key=lambda x: severity_order.get(x["severity"], 3))
        
        return recommendations
    
    def export_metrics(
        self,
        format: str = "json",
        time_window_seconds: Optional[int] = None
    ) -> str:
        """
        å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡
        
        Args:
            format: å¯¼å‡ºæ ¼å¼ï¼ˆjsonæˆ–csvï¼‰
            time_window_seconds: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            str: å¯¼å‡ºçš„æ•°æ®
        """
        # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®
        data = {
            "export_time": datetime.now().isoformat(),
            "time_window_seconds": time_window_seconds,
            "tool_statistics": self.get_tool_statistics(time_window_seconds=time_window_seconds),
            "dag_statistics": self.get_dag_statistics(time_window_seconds=time_window_seconds),
            "llm_statistics": self.get_llm_statistics(time_window_seconds=time_window_seconds),
            "cache_statistics": self.get_cache_statistics(time_window_seconds=time_window_seconds),
            "performance_snapshot": asdict(self.get_performance_snapshot(time_window_seconds or 3600)),
            "optimization_recommendations": self.get_optimization_recommendations()
        }
        
        if format == "json":
            return json.dumps(data, indent=2, ensure_ascii=False, default=str)
        elif format == "csv":
            # ç®€åŒ–çš„CSVå¯¼å‡ºï¼ˆä»…åŒ…å«å…³é”®æŒ‡æ ‡ï¼‰
            lines = ["metric,value"]
            lines.append(f"total_dag_executions,{len(self.dag_executions)}")
            lines.append(f"total_llm_calls,{len(self.llm_calls)}")
            lines.append(f"total_tool_executions,{len(self.tool_executions)}")
            return "\n".join(lines)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
    
    def reset_statistics(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡æ•°æ®"""
        self.tool_executions.clear()
        self.dag_executions.clear()
        self.llm_calls.clear()
        self.cache_metrics_history.clear()
        self.tool_stats.clear()
        self.llm_stats.clear()
        self.error_counts.clear()
        
        logger.info("ğŸ”„ æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _calculate_percentile(self, data: List[float], percentile: float) -> float:
        """
        è®¡ç®—ç™¾åˆ†ä½æ•°
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            percentile: ç™¾åˆ†ä½ï¼ˆ0-1ï¼‰
            
        Returns:
            float: ç™¾åˆ†ä½å€¼
        """
        if not data:
            return 0.0
        
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile)
        index = min(index, len(sorted_data) - 1)
        return sorted_data[index]


# å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹
_global_performance_monitor: Optional[PerformanceMonitor] = None


def get_performance_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹"""
    global _global_performance_monitor
    if _global_performance_monitor is None:
        _global_performance_monitor = PerformanceMonitor()
    return _global_performance_monitor


def set_performance_monitor(monitor: PerformanceMonitor):
    """è®¾ç½®å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹"""
    global _global_performance_monitor
    _global_performance_monitor = monitor
